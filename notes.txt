1. Single Tier Applications
- A single-tier application is an application where the user interface, backend business logic & the database all reside in the same machine.
- The main upside of single-tier applications is they have no network latency since every component is located on the same machine. This adds up to the performance of the software.

2. Two Tier Application 
- A Two-tier application involves a client and a server. The client would contain the user interface & the business logic in one machine. And the backend server would be the database running on a different machine. The database server is hosted by the business & has control over it.

3. Three Tier applications
- In a three-tier application, the user interface, application logic & the database all lie on different machines & thus have different tiers. They are physically separated.

4. N Tier Applications
- An N-tier application is an application which has more than three components involved. For example 
  - Cache
  - Message queues for asynchronous behaviour
  - Load balancers
  - Search servers for searching through massive amounts of data
  - Components involved in processing massive amounts of data
  - Components running heterogeneous tech commonly known as web services etc.
- Note: There is another name for n-tier apps, the “distributed applications”. But, I think it’s not safe to use the word “distributed” yet, as the term distributed brings along a lot of complex stuff with it. It would rather confuse us than help. Though I will discuss the distributed architecture in this course, for now, we will just stick with the term N-tier applications.
- Why the need for so many tiers? 
- Two software design principles that are key to explaining this are the Single Responsibility Principle & the Separation of Concerns.
- Single Responsibility Principle simply means giving one, just one responsibility to a component & letting it execute it with perfection. Be it saving data, running the application logic or ensuring the delivery of the messages throughout the system.
- Separation of concerns kind of means the same thing, be concerned about your work only & stop worrying about the rest of the stuff. Keeping the components separate makes them reusable. Different services can use the same database, the messaging server or any component as long as they are not tightly coupled with each other.
- Note: Don’t confuse tiers with the layers of the application. Some prefer to use them interchangeably. But in the industry layers of an application typically means the user interface layer, business layer, service layer, or the data access layer.

5. What Is Web Architecture?
- Web architecture involves multiple components like database, message queue, cache, user interface & all running in conjunction with each other to form an online service.

6. Client Server Architecture
- The architecture works on a request-response model. The client sends the request to the server for information & the server responds with it.
- A very small percent of the business websites and applications use the peer to peer architecture, which is different from the client-server.
- Every website you browse, be it a Wordpress blog or a web application like Facebook, Twitter or your banking app is built on the client-server architecture.

7. Client
- The client holds our user interface. The user interface is the presentation part of the application. It’s written in Html, JavaScript, CSS and is responsible for the look & feel of the application.
- The user interface runs on the client. The client can be a mobile app, a desktop or a tablet like an iPad. It can also be a web-based console, running commands to interact with the backend server.

8. Types of Client
- Thin Client & Thick Client
- Thin Client is the client which holds just the user interface of the application. It has no business logic of any sort. For every action, the client sends a request to the backend server. Just like in a three-tier application.
- On the contrary, the thick client holds all or some part of the business logic. These are the two-tier applications. We’ve already gone through this if you remember.

9. Server
- The primary task of a web server is to receive the requests from the client & provide the response after executing the business logic based on the request parameters received from the client.
- Every service, running online, needs a server to run. Servers running web applications are commonly known as the application servers.
- Besides the application servers, there are other kinds of servers too with specific tasks assigned to them such as the:	
    Proxy server
    Mail server
    File server
    Virtual server
- All the components of a web application need a server to run. Be it a database, a message queue, a cache or any other component. In modern application development, even the user interface is hosted separately on a dedicated server.

10. Communication Between the Client & the Server
- The client & the server have a request-response model. The client sends the request & the server responds with the data.
- The entire communication happens over the HTTP protocol. It is the protocol for data exchange over the World Wide Web. HTTP protocol is a request-response protocol that defines how information is transmitted across the web.
- It’s a stateless protocol, every process over HTTP is executed independently & has no knowledge of previous processes.
- Speaking from the context of modern N-tier web applications, every client has to hit a REST end-point to fetch the data from the backend.
- The backend application code has a REST-API implemented which acts as an interface to the outside world requests. Every request be it from the client written by the business or the third-party developers which consume our data have to hit the REST-endpoints to fetch the data.

11. HTTP Push & Pull - Introduction
- For every response, there has to be a request first. The client sends the request & the server responds with the data. This is the default mode of HTTP communication, called the HTTP PULL mechanism.    
- In HTTP PUSH based mechanism the client sends the request for particular information to the server, just for the first time, & after that the server keeps pushing the new updates to the client whenever they are available. The client doesn’t have to worry about sending requests to the server, for data, every now & then. This saves a lot of network bandwidth & cuts down the load on the server by notches. This is also known as a Callback. Client phones the server for information. The server responds, Hey!! I don’t have the information right now but I’ll call you back whenever it is available.
- There are multiple technologies involved in the HTTP Push based mechanism such as:	
    Ajax Long polling
    Web Sockets
    HTML5 Event Source
    Message Queues
    Streaming over HTTP

12. HTTP Pull - Polling with Ajax
- There are two ways of pulling/fetching data from the server. The first is sending an HTTP GET request to the server manually by triggering an event, like by clicking a button or any other element on the web page.
- The other is fetching data dynamically at regular intervals by using AJAX without any human intervention. AJAX is commonly used with the Jquery framework to implement the asynchronous behaviour on the UI. 
- This dynamic technique of requesting information from the server after regular intervals is known as Polling.

13. HTTP Push
- The above mechanism has the following
    Time To Live (TTL)
    Persistent Connection
    Heartbeat Interceptors
    Resource Intensive
- In the regular client-server communication, which is HTTP PULL, there is a Time to Live (TTL) for every request. It could be 30 secs to 60 secs, varies from browser to browser.
- If the client doesn’t receive a response from the server within the TTL, the browser kills the connection & the client has to re-send the request hoping it would receive the data from the server before the TTL ends this time.
- Open connections consume resources & there is a limit to the number of open connections a server can handle at one point in time. If the connections don’t close & new ones are being introduced, over time, the server will run out of memory. Hence, the TTL is used in client-server communication.
- But what if we are certain that the response will take more time than the TTL set by the browser?. In this case, we need a Persistent Connection between the client and the server. 
- A persistent connection is a network connection between the client & the server that remains open for further requests & the responses, as opposed to being closed after a single communication. It facilitates HTTP Push-based communication between the client and the server.    
- Now you might be wondering how is a persistent connection possible if the browser kills the open connections to the server every X seconds?. The connection between the client and the server stays open with the help of Heartbeat Interceptors.
- These are just blank request responses between the client and the server to prevent the browser from killing the connection.
- Persistent connections consume a lot of resources in comparison to the HTTP Pull behaviour. But there are use cases where establishing a persistent connection is vital to the feature of an application.For instance, a browser-based multiplayer game has a pretty large amount of request-response activity within a certain time in comparison to a regular web application.
- It would be apt to establish a persistent connection between the client and the server from a user experience standpoint.
- Long opened connections can be implemented by multiple techniques such as Ajax Long Polling, Web Sockets, Server-Sent Events etc.

14. HTTP Push-Based Technologies
- The below are the HTTP Push-Based technologies  
    Web Sockets
    AJAX – Long Polling
    HTML5 Event Source API & Server Sent Events
    Streaming Over HTTP
- A Web Socket connection is ideally preferred when we need a persistent bi-directional low latency data flow from the client to server & back.
- Typical use-cases of these are messaging, chat applications, real-time social streams & browser-based massive multiplayer games which have quite a number of read writes in comparison to a regular web app.
- With Web Sockets, we can keep the client-server connection open as long as we want. Have bi-directional data? Go ahead with Web Sockets. One more thing, Web Sockets tech doesn’t work over HTTP. It runs over TCP. The server & the client should both support web sockets or else it won’t work.
- Long Polling lies somewhere between Ajax & Web Sockets. In this technique instead of immediately returning the response, the server holds the response until it finds an update to be sent to the client. The connection in long polling stays open a bit longer in comparison to polling. The server doesn’t return an empty response. If the connection breaks, the client has to re-establish the connection to the server. The upside of using this technique is that there are quite a smaller number of requests sent from the client to the server, in comparison to the regular polling mechanism. This reduces a lot of network bandwidth consumption. Long polling can be used in simple asynchronous data fetch use cases when you do not want to poll the server every now & then.
- The Server-Sent Events implementation takes a bit of a different approach. Instead of the client polling for data, the server automatically pushes the data to the client whenever the updates are available. The incoming messages from the server are treated as Events. Via this approach, the servers can initiate data transmission towards the client once the client has established the connection with an initial request.
- This helps in getting rid of a huge number of blank request-response cycles cutting down the bandwidth consumption by notches.
- To implement server-sent events, the backend language should support the technology & on the UI HTML5 Event source API is used to receive the data in-coming from the backend.
- An important thing to note here is that once the client establishes a connection with the server, the data flow is in one direction only, that is from the server to the client.
- SSE is ideal for scenarios such as a real-time feed like that of Twitter, displaying stock quotes on the UI, real-time notifications etc.
- Streaming Over HTTP is ideal for cases where we need to stream large data over HTTP by breaking it into smaller chunks. This is possible with HTML5 & a JavaScript Stream API.
- The technique is primarily used for streaming multimedia content, like large images, videos etc, over HTTP. Due to this, we can watch a partially downloaded video as it continues to download, by playing the downloaded chunks on the client. This helps them figure when the stream begins & ends over an HTTP request-response model.

15. Client-Side Vs Server-Side Rendering
- When a user requests a web page from the server & the browser receives the response. It has to render the response on the window in the form of an HTML page. For this, the browser has several components, such as the:
    Browser engine
    Rendering engine
    JavaScript interpreter
    Networking & the UI backend
    Data storage etc.
- The rendering engine constructs the DOM tree, renders & paints the construction. And naturally, all this activity needs a bit of time.
- To avoid all this rendering time on the client, developers often render the UI on the server, generate HTML there & directly send the HTML page to the UI.
- This technique is known as the Server-side rendering. It ensures faster rendering of the UI, averting the UI loading time in the browser window since the page is already created & the browser doesn’t have to do much assembling & rendering work.
- The server-side rendering approach is perfect for delivering static content, such as WordPress blogs. It’s also good for SEO as the crawlers can easily read the generated content.
- However, the modern websites are highly dependent on Ajax. In such websites, content for a particular module or a section of a page has to be fetched & rendered on the fly. Therefore, server-side rendering doesn’t help much. For every Ajax-request, instead of sending just the required content to the client, the approach generates the entire page on the server. This process consumes unnecessary bandwidth & also fails to provide a smooth user experience.
- A big downside to this is once the number of concurrent users on the website rises, it puts an unnecessary load on the server.
- Client-side rendering works best for modern dynamic Ajax-based websites. Though we can leverage a hybrid approach, to get the most out of both techniques. We can use server-side rendering for the home page & for the other static content on our website & use client-side rendering for the dynamic pages.

16. What is Scalability?
- Scalability means the ability of the application to handle & withstand increased workload without sacrificing the latency.
- Latency is the amount of time a system takes to respond to a user request. Let’s say you send a request to an app to fetch an image & the system takes 2 seconds to respond to your request. The latency of the system is 2 seconds.
- Minimum latency is what efficient software systems strive for. No matter how much the traffic load on a system builds up, the latency should not go up. This is what scalability is.
- If the latency remains the same, we can say yeah, the application scaled well with the increased load & is highly scalable.
- Latency is measured as the time difference between the action that the user takes on the website, it can be an event like the click of a button, & the system response in reaction to that event.
- This latency is generally divided into two parts:
    Network Latency
    Application Latency
- Network Latency is the amount of time that the network takes for sending a data packet from point A to point B. The network should be efficient enough to handle the increased traffic load on the website. To cut down the network latency, businesses use CDN & try to deploy their servers across the globe as close to the end-user as possible.
- Application Latency is the amount of time the application takes to process a user request. There are more than a few ways to cut down the application latency. The first step is to run stress & load tests on the application & scan for the bottlenecks that slow down the system as a whole. I’ve talked more about it in the upcoming lesson.
- We can realize the importance of low latency by the fact that Huawei & Hibernia Atlantic in the year 2011 started laying a fibre-optic link cable across the Atlantic Ocean between London & Newyork, that was estimated having a cost of approx. $300M, just to save traders 6 milliseconds of latency.

17. Types of Scalability
- Vertical Scaling
- Horizontal Scaling
- Cloud Elasiticity: The biggest reason why cloud computing got so popular in the industry is the ability to scale up & down dynamically. The ability to use & pay only for the resources required by the website became a trend for obvious reasons.
- Having multiple server nodes on the backend also helps with the website staying alive online all the time even if a few server nodes crash. This is known as High Availability.

18. Which Scalability Approach Is Right For Your App?
- Vertical scaling for obvious reasons is simpler in comparison to scaling horizontally as we do not have to touch the code or make any complex distributed system configurations. It takes much less administrative, monitoring, management efforts as opposed to when managing a distributed environment.
- A major downside of vertical scaling is availability risk. The servers are powerful but few in number, there is always a risk of them going down & the entire website going offline which doesn’t happen when the system is scaled horizontally. It becomes more highly available.
- If you need to run the code in a distributed environment, it needs to be stateless. There should be no state in the code.
- No static instances in the class. Static instances hold application data & if a particular server goes down all the static data/state is lost. The app is left in an inconsistent state.
- Rather, use a persistent memory like a key-value store to hold the data & to remove all the state/static variable from the class. This is why functional programming got so popular with distributed systems. The functions don’t retain any state.
- Development teams today are adopting a distributed micro-services architecture right from the start & the workloads are meant to be deployed on the cloud. So, inherently the workloads are horizontally scaled out on the fly.

19. Primary Bottlenecks that Hurt the Scalability Of Our Application
    Database
    Application Architecture
    Not Using Caching In the Application Wisely
    Inefficient Configuration & Setup of Load Balancers
    Adding Business Logic to the Database
    Not Picking the Right Database
    At the Code Level 
- Just one server been given the onus of handling the data requests from all the server nodes of the workload. This scenario is a bottleneck. The server nodes work well, handle millions of requests at a point in time efficiently, still, the response time of these requests & the latency of the application is very high due to the presence of a single database. There is only so much it can handle.
- Just like the workload scalability, the database needs to be scaled well.
- Make wise use of database partitioning, sharding, use multiple database servers to make the module efficient.
- A poorly designed application’s architecture can become a major bottleneck as a whole. A common architectural mistake is not using asynchronous processes & modules where ever required rather all the processes are scheduled sequentially.
- For instance, if a user uploads a document on the portal, tasks such as sending a confirmation email to the user, sending a notification to all of the subscribers/listeners to the upload event should be done asynchronously.
- These tasks should be forwarded to a messaging server as opposed to doing it all sequentially & making the user wait for everything.
- Caching can be deployed at several layers of the application & it speeds up the response time by notches. It intercepts all the requests going to the database, reducing the overall load on it.
- Use caching exhaustively throughout the application to speed up things significantly.
- Load balancers are the gateway to our application. Using too many or too few of them impacts the latency of our application.
- No matter what justification anyone provides, I’ve never been a fan of adding business logic to the database.
- The database is just not the place to put business logic. Not only it makes the whole application tightly coupled. It puts unnecessary load on it.
- Imagine when migrating to a different database, how much code refactoring it would require.
- Picking the right database technology is vital for businesses. Need transactions & strong consistency? Pick a Relational Database. If you can do without strong consistency rather need horizontal scalability on the fly pick a NoSQL database.
- Trying to pull off things with a not so suitable tech always has a profound impact on the latency of the entire application in negative ways.
- Using unnecessary loops, nested loops.
- Writing tightly coupled code.
- Not paying attention to the Big-O complexity while writing the code. Be ready to do a lot of firefighting in production.

20. How To Improve & Test the Scalability Of Our Application?
    Tuning The Performance Of The Application – Enabling It To Scale Better
        Profiling
        Caching
        CDN (Content Delivery Network)
        Data Compression
        Avoid Unnecessary Client Server Requests
    Testing the Scalability Of Our Application
- The application’s performance is directly proportional to scalability. If an application is not performant it will certainly not scale well.
- Run application profiler, code profiler. See which processes are taking too long, eating up too much resources. Find out the bottlenecks. Get rid of them. Profiling is the dynamic analysis of our code. It helps us measure the space and the time complexity of our code & enables us to figure out issues like concurrency errors, memory errors & robustness & safety of the program.
- Cache wisely. Cache everywhere. Cache all the static content. Hit the database only when it is really required. Try to serve all the read requests from the cache. Use a write-through cache.
- Use a CDN. Using a CDN further reduces the latency of the application due to the proximity of the data from the requesting user.
- Compress data. Use apt compression algorithms to compress data. Store data in the compressed form. As compressed data consumes less bandwidth, consequently, the download speed of the data on the client will be faster.
- Avoid unnecessary round trips between the client & server. Try to club multiple requests into one.
- During the scalability testing, different system parameters are taken into account such as the CPU usage, network bandwidth consumption, throughput, the number of requests processed within a stipulated time, latency, memory usage of the program, end-user experience when the system is under heavy load etc.
- In this testing phase, simulated traffic is routed to the system, to study how the system behaves under the heavy load, how the application scales under the heavy load. Contingencies are planned for unforeseen situations.
- Several load & stress tests are run on the application. Tools like JMeter are pretty popular for running concurrent user test on the application if you are working on a Java ecosystem. There are a lot of cloud-based testing tools available that help us simulate tests scenarios just with a few mouse clicks.
- In the industry tech like Cadvisor, Prometheus and Grafana are pretty popular for tracking the system via web-based dashboards.

21. What Is High Availability?
- High availability also known as HA is the ability of the system to stay online despite having failures at the infrastructural level in real-time. High availability ensures the uptime of the service much more than the normal time. It improves the reliability of the system, ensures minimum downtime. 
- To meet the high availability requirements systems are designed to be fault-tolerant, their components are made redundant.

22. Reasons For System Failures
    Software Crashes
    Hardware Failures
    Human Errors
    Planned Downtime
- Software running on cloud nodes crash unpredictably, along with it they take down the entire node.     
- Overloaded CPU, RAM, hard disk failures, nodes going down. Network outages.
- Google made a tiny network configuration error & it took down almost half of the internet in Japan.
- Besides the unplanned crashes, there are planned down times which involve routine maintenance operations, patching of software, hardware upgrades etc.

23. Achieving High Availability - Fault Tolerance
- Fault tolerance is the ability of the system to stay up despite taking hits.
- A fault-tolerant system is equipped to handle faults. Being fault-tolerant is an essential element in designing life-critical systems.
- A few of the instances/nodes, out of several, running the service go offline & bounce back all the time. In case of these internal failures, the system could work at a reduced level but it will not go down entirely.
- A very basic example of a system being fault-tolerant is a social networking application. In the case of backend node failures, a few services of the app such as image upload, post likes etc. may stop working. But the application as a whole will still be up. This approach is also technically known as Fail Soft.
- To achieve high availability at the application level, the entire massive service is architecturally broken down into smaller loosely coupled services called the micro-services.

24. Redundancy

    Redundancy – Active-Passive HA Mode
    Getting Rid Of Single Points Of Failure
    Monitoring & Automation

- Redundancy is duplicating the components or instances & keeping them on standby to take over in case the active instances go down. It’s the fail-safe, backup mechanism.
- In the diagram, you can see the instances active & on standby. The standby instances take over in case any of the active instances goes down.
- This approach is also known as Active-Passive HA mode. An initial set of nodes are active & a set of redundant nodes are passive, on standby. Active nodes get replaced by passive nodes, in case of failures.
- There are systems like GPS, aircrafts, communication satellites which have zero downtime. The availability of these systems is ensured by making the components redundant.
- Distributed systems got so popular solely due to the reason that with them, we could get rid of the single points of failure present in a monolithic architecture.
- A large number of distributed nodes work in conjunction with each other to achieve a single synchronous application state.
- When so many redundant nodes are deployed, there are no single points of failure in the system. In case a node goes down redundant nodes take its place. Thus, the system as a whole remains unimpacted.
- Single points of failure at the application level mean bottlenecks. We should detect bottlenecks in performance testing & get rid of them as soon as we can.
- Systems should be well monitored in real-time to detect any bottlenecks or single point of failures. Automation enables the instances to self-recover without any human intervention. It gives the instances the power of self-healing.
- Also, the systems become intelligent enough to add or remove instances on the fly as per the requirements.
- Since the most common cause of failures is human error, automation helps cut down failures to a big extent.

25. Replication

	Replication – Active-Active HA Mode
	Geographical Distribution of Workload

- Replication means having a number of similar nodes running the workload together. There are no standby or passive instances. When a single or a few nodes go down, the remaining nodes bear the load of the service. Think of this as load balancing.	
- This approach is also known as the Active-Active High Availability mode. In this approach, all the components of the system are active at any point in time.
- As a contingency for natural disasters, data centre regional power outages & other big-scale failures, workloads are spread across different data centres across the world in different geographical zones.
- This avoids the single point of failure thing in context to a data centre. Also, the latency is reduced by quite an extent due to the proximity of data to the user.
- Businesses often use multi-cloud platforms to deploy their workloads which ensures further availability. If things go south with one cloud provider, they have another to fail back over.

26. High Availability Clustering
- A High Availability cluster also known as the Fail-Over cluster contains a set of nodes running in conjunction with each other that ensures high availability of the service.
- The nodes in the cluster are connected by a private network called the Heartbeat network that continuously monitors the health and the status of each node in the cluster.
- A single state across all the nodes in a cluster is achieved with the help of a shared distributed memory & a distributed co-ordination service like the Zookeeper.
- To ensure the availability, HA clusters use several techniques such as Disk mirroring/RAID Redundant Array Of Independent Disks, redundant network connections, redundant electrical power etc. The network connections are made redundant so if the primary network goes down the backup network takes over.
- Multiple HA clusters run together in one geographical zone ensuring minimum downtime & continual service.

27. Introduction To Load Balancing
- Load balancing is vital in enabling our service to scale well, with the increase in the traffic load, as well as stay highly available. Load balancing is facilitated by load balancers, that makes them a key component in the web application architecture.
- Load balancers distribute heavy traffic load across the servers running in the cluster based on several different algorithms. This averts the risks of convergence of all the traffic on the service to a single or a few machines in the cluster.
- If the entire traffic on the service is converged only to a few machines this will not only overload them resulting in the increase in the latency of the application, killing its performance. But will also eventually bring them down.
- Load balancing helps us avoid all this mess. Amidst processing a user request if a server goes down, the load balancer automatically routes the future requests to other up and running servers in the cluster thus enabling the service as a whole to stay available.
- Load balancers can also be setup to efficiently manage traffic directed towards any component of the application be it the backend application server, database component, message queue or any other. This is done to uniformly spread the request load across the machines in the clusters powering that respective component.
- In order to intelligently route all the user requests to the running servers in the cluster, a load balancer should be well aware of their running status.
- To ensure that the user request is always routed to the machine that is up and running, load balancers regularly perform health checks of the machines in the cluster.
- Ideally, a load balancer maintains a list of machines that are up and running in the cluster in real-time & the user requests are forwarded to only those machines that are in service. If a machine goes down it is removed from the list.
- Machines that are up and running in the cluster are known as in-service machines. And the servers that are down are known as out of service instances.
- Just for the record – Node, Server, Server Node, Instance, Machine they all mean the same thing & can be used interchangeably.

28. Understanding DNS 
- Every machine that is online & is a part of world wide web www has a unique IP address that enables it to be contacted by other machines on the web using that particular IP address.
- Domain name system commonly known as DNS is a system that averts the need to remember long IP addresses to visit a website by mapping easy to remember domain names to IP addresses.
- When a user types in the url of the website in their browser and hits enter. This event is known as DNS querying.
- There are four key components, i.e. a group of servers, that make up the DNS infrastructure. Those are -

    DNS Recursive Nameserver aka DNS Resolver
    Root Nameserver
    Top-Level Domain Nameserver
    Authoritative Nameserver
- So, when the user hits enter after typing in the domain name into their browser, the browser sends a request to the DNS Recursive nameserver which is also known as the DNS Resolver.
- The role of DNS Resolver is to receive the client request and forward it to the Root nameserver to get the address of the Top-Level domain nameserver.
- DNS Recursive nameserver is generally managed by our ISP Internet service provider. The whole DNS system is a distributed system setup in large data centers managed by internet service providers.
- These data centers contain clusters of servers that are optimized to process DNS queries in minimal time that is in milliseconds.
- So, once the DNS Resolver forwards the request to the Root nameserver, the Root nameserver returns the address of the Top-Level domain nameserver in response. As an example, the top-level domain for amazon.com is .com.
- Once the DNS Resolver receives the address of the top-level domain nameserver, it sends a request to it to fetch the details of the domain name. Top level domain nameservers hold the data for domains using their top-level domains.
- For instance, .com top-level domain nameserver will contain information on domains using .com. Similarly, a .edu top-level domain nameserver will hold information on domains using .edu.
- Once the top-level domain name server receives the request from the Resolver, it returns the IP address of amazon.com domain name server.
- amazon.com domain name server is the last server in the DNS query lookup process. It is the nameserver responsible for the amazon.com domain & is also known as the Authoritative nameserver. This nameserver is owned by the owner of the domain name.
- DNS Resolver then fires a query to the authoritative nameserver & it then returns the IP address of amazon.com website to the DNS Resolver. DNS Resolver caches the data and forwards it to the client.
- On receiving the response, the browser sends a request to the IP address of the amazon.com website to fetch data from their servers.
- Often all this DNS information is cached and the DNS servers don’t have to do so much rerouting every time a client requests an IP of a certain website.
- DNS information of websites that we visit also gets cached in our local machines that is our browsing devices with a TTL Time To Live.
- All the modern browsers do this automatically to cut down the DNS query lookup time when revisiting a website.

29. DNS Load Balancing
- When a large-scale service such as amazon.com runs, it needs way more than a single machine to run its services. A service as big as amazon.com is deployed across multiple data centers in different geographical locations across the globe.
- DNS load balancing enables the authoritative server to return different IP addresses of a certain domain to the clients. Every time it receives a query for an IP, it returns a list of IP addresses of a domain to the client.
- With every request, the authoritative server changes the order of the IP addresses in the list in a round-robin fashion.
- As the client receives the list, it sends out a request to the first IP address on the list to fetch the data from the website. The reason for returning a list of IP addresses to the client is to enable it to use other IP addresses in the list in case the first doesn’t return a response within a stipulated time.
- When another client sends out a request for an IP address to the authoritative server, it re-orders the list and puts another IP address on the top of the list following the round-robin algorithm.
- Also, when the client hits an IP it may not necessarily hit an application server, it may hit another load balancer implemented at the data center level that manages the clusters of application servers.
- DNS load balancing is largely used by companies to distribute traffic across multiple data centers that the application runs in. Though this approach has several limitations, for instance, it doesn’t take into account the existing load on the servers, the content they hold, their request processing time, their in-service status and so on.
- Also, since these IP addresses are cached by the client’s machine and the DNS Resolver, there is always a possibility of a request being routed to a machine that is out of service.
- DNS load balancing despite its limitations is preferred by companies because it’s an easy and less expensive way of setting up load balancing on their service.

30. Load Balancing Methods

	Hardware Load Balancers
	Software Load Balancers
	Algorithms/Traffic Routing Approaches Leveraged By Load Balancers
	    Round Robin & Weighted Round Robin
	    Least Connections
	    Random
	    Hash
There are primarily three modes of load balancing -
    DNS Load Balancing
    Hardware-based Load Balancing
    Software-based Load Balancing
- Hardware load balancers are highly performant physical hardware, they sit in front of the application servers and distribute the load based on the number of existing open connections to a server, compute utilization and several other parameters.
- Since, these load balancers are physical hardware they need maintenance & regular updates, just like any other server hardware would need. They are expensive to setup in comparison to software load balancers and their upkeep may require a certain skill set.
- If the business has an IT team & network specialists in house, they can take care of these load balancers else the developers are expected to wrap their head around how to set up these hardware load balancers with some assistance from the vendor. This is the reason developers prefer working with software load balancers.
- When using hardware load balancers, we may also have to overprovision them to deal with the peak traffic that is not the case with software load balancers.
- Hardware load balancers are primarily picked because of their top-notch performance.
- Software load balancers can be installed on commodity hardware and VMs. They are more cost-effective and offer more flexibility to the developers. Software load balancers can be upgraded and provisioned easily in comparison to hardware load balancers.
- You will also find several LBaaS Load Balancers as a Service services online that enable you to directly plug in load balancers into your application without you having to do any sort of setup.
- Software load balancers are pretty advanced when compared to DNS load balancing as they consider many parameters such as content that the servers host, cookies, HTTP headers, CPU & memory utilization, load on the network & so on to route traffic across the servers.
- They also continually perform health checks on the servers to keep an updated list of in-service machines
- HAProxy is one example of a software load balancer that is widely used by the big guns in the industry to scale their systems such as GitHub, Reddit, Instagram, AWS, Tumblr, StackOverflow & many more.
- Besides the Round Robin algorithm that the DNS Load balancers use, software load balancers leverage several other algorithms to efficiently route traffic across the machines. Let’s have an insight.
- We know that Round Robin algorithm sends IP address of machines sequentially to the clients. Parameters such as load on the servers, their CPU consumption and so on are not taken into account when sending the IP addresses to the clients.
- We have another approach known as the Weighted Round Robin where based on the server’s compute & traffic handling capacity weights are assigned to them. And then based on the server weights, traffic is routed to them using the Round Robin algorithm.
- This approach is pretty useful when the service is deployed in different data centers having different compute capacities. More traffic can be directed to the larger data centers containing more machines.
- Least Connections When using this algorithm, the traffic is routed to the machine that has the least open connections of all the machines in the cluster. There are two approaches to implement this     
- In the first, it is assumed that all the requests will consume an equal amount of server resources & the traffic is routed to the machine, having the least open connections, based on this assumption.
- Now in this scenario, there is a possibility that the machine having the least open connections might be already processing requests demanding most of its CPU power. Routing more traffic to this machine wouldn’t be such a good idea.
- In the other approach, the CPU utilization & the request processing time of the chosen machine is also taken into account before routing the traffic to it. Machines with less request processing time, CPU utilization & simultaneously having the least open connections are the right candidates to process the future client requests.
- The least connections approach comes in handy when the server has long opened connections, for instance, persistent connections in a gaming application.
- Random Following this approach, the traffic is randomly routed to the servers. The load balancer may also find similar servers in terms of existing load, request processing time and so on and then it randomly routes the traffic to these machines.
- Hash In this approach, the source IP where the request is coming from and the request URL are hashed to route the traffic to the backend servers.
- Hashing the source IP ensures that the request of a client with a certain IP will always be routed to the same server.
- This facilitates better user experience as the server has already processed the initial client requests and holds the client’s data in its local memory. There is no need for it to fetch the client session data from the session memory of the cluster & then process the request. This reduces latency.
- Hashing the client IP also enables the client to re-establish the connection with the same server, that was processing its request, in case the connection drops.
- Hashing a url ensures that the request with that url always hits a certain cache that already has data on it. This is to ensure that there is no cache miss.
- This also averts the need for duplicating data in every cache and is thus a more efficient way to implement caching.

31. What Is A Monolithic Architecture? 
- An application has a monolithic architecture if it contains the entire application code in a single codebase.
- In a monolithic web-app all the different layers of the app, UI, business, data access etc. are in the same codebase.
- Monolithic apps are simple to build, test & deploy in comparison to a microservices architecture.
- Monolithic applications are simple to develop, test, deploy, monitor and manage since everything resides in one repository.
- Continuous deployment is a pain in case of monolithic applications as even a minor code change in a layer needs a re-deployment of the entire application.
- The downside of this is that we need a thorough regression testing of the entire application after the deployment is done as the layers are tightly coupled with each other. A change in one layer impacts other layers significantly.
- Monolithic applications have a single point of failure. In case any of the layers has a bug, it has the potential to take down the entire application.
- Flexibility and scalability are a challenge in monolith apps as a change in one layer often needs a change and testing in all the layers. As the code size increases, things might get a bit tricky to manage.
- Building complex applications with a monolithic architecture is tricky as using heterogeneous technologies is difficult in a single codebase due to the compatibility issues.
- Generally, monolithic applications are not cloud-ready as they hold state in the static variables. An application to be cloud-native, to work smoothly & to be consistent on the cloud has to be distributed and stateless.
- Monolithic applications fit best for use cases where the requirements are pretty simple, the app is expected to handle a limited amount of traffic. One example of this is an internal tax calculation app of an organization or a similar open public tool.

32. What Is A Microservice Architecture?
- In a microservices architecture, different features/tasks are split into separate respective modules/codebases which work in conjunction with each other forming a large service as a whole.
- This particular architecture facilitates easier & cleaner app maintenance, feature development, testing & deployment in comparison to a monolithic architecture.
- Microservices architecture is inherently designed to scale. Every service ideally has a separate database, there are no single points of failure & system bottlenecks.
- Since microservices is a loosely coupled architecture, there is no single point of failure. Even if a few of the services go down, the application as a whole is still up.
- Every component interacts with each other via a REST API Gateway interface. The components can leverage the polyglot persistence architecture & other heterogeneous technologies together like Java, Python, Ruby, NodeJS etc.
- Polyglot persistence is using multiple databases types like SQL, NoSQL together in an architecture.
- The deployments can be independent and continuous. We can have dedicated teams for every microservice, it can be scaled independently without impacting other services.
- Microservices is a distributed environment, where there are so many nodes running together. Managing & monitoring them gets complex.
- We need to setup additional components to manage microservices such as a node manager like Apache Zookeeper, a distributed tracing service for monitoring the nodes etc.
- Strong consistency is hard to guarantee in a distributed environment. Things are Eventually consistent across the nodes. And this limitation is due to the distributed design.

33. Monolith & Microservices – Understanding The Trade-Offs 

    Fault Isolation
    Development Team Autonomy
    Segment – From Monolith To Microservices And Back Again To Monolith
- When we have a microservices architecture in place it’s easy for us to isolate faults and debug them. Even if the service goes down due to the fault, the other services are up & running. This has a minimal impact on the system.
- Segment is a customer data platform that originally started with a monolith and then later split it into microservices. As their business gained traction, they again decided to revert to the monolith architecture.    
- Segment integrates data from many different data providers into their system. As the business gained traction, they integrated more data providers into their system creating a separate microservice for every data provider. The increase in the number of microservices led to an increase in the complexity of their architecture significantly, subsequently taking a toll on their productivity. The defects with regards to microservices started increasing significantly. They had three engineers straight up just dedicated to getting rid of these defects to keep the system online. This operational overhead became resource-intensive & slowed down the organization immensely. To tackle the issue, they made the decision to move back to monolith giving up on fault isolation and other nice things that the microservices architecture brought along. They ended up with an architecture having a single code repository that they called Centrifuge that handled billions of messages per day delivered to multiple APIs.

34. Introduction To Micro Frontends
- Micro frontends are separate loosely coupled components of the user interface of an application that are developed applying the principles of microservices on the front end.
- Writing micro frontends is more of an architectural design decision & a development approach as opposed to it being a technology.
- Micro frontends offer the same upsides as microservices to the front-end development.
- Generally, in application development, despite having a microservices architecture on the backend, our front end is a monolith that is developed by a dedicated front-end development team.
- But with the micro frontends approach, we split our application into vertical slices. Where a single slice goes end to end right from the user interface to the database.
- Every slice is owned by a dedicated team. The team besides the backend devs also includes the front-end developers who have the onus of developing the user interface component only of that particular service.
- Every team builds its own user interface component choosing their desired technology and later all these components are integrated together forming the complete user interface of the application. This micro frontend approach averts the need for a dedicated centralized user interface team.
- Micro frontends approach is pretty popular with e-commerce websites. 
- When we have full-stack teams owning an entire service end to end, this averts the need for a dedicated front-end team. Since the front end devs now work along with the backend devs in the same team this saves a lot of time that was initially spent in the cross-team coordination that was between the backend microservices and the dedicated front-end teams.
- Since, the micro frontends are loosely coupled, just like microservices, we can develop them leveraging different technologies. This lets us off the hook of sticking to just one UI technology to build the entire front end of the website.
- Going forward with the micro frontends approach may sound delightful but it’s only fit for medium to large websites. This approach won’t be that advantageous for simple use cases. Rather it will make things more complex.
- Using multiple technologies in a project brings along a lot of architectural & maintenance complexities with it. With micro frontends, we also need to write additional logic to club all the components together.
- We also cannot ignore the compatibility & performance issues when using multiple technologies together. So, there are always trade-offs involved. There is no silver bullet.
- So, once we have different micro frontends ready for our store, we need to integrate them together to have a functional website. There are two ways we can do this -
	1. By integrating micro frontends on the client
	2. By integrating micro frontends on the server
- This concept is pretty similar to the client side & the server-side rendering, in this micro frontends use case, we need to write additional logic to enable the integration of the UI components.	
- A very basic naïve way of integrating components on the client is having micro frontends with unique links. We just place the links on the website to enable the user to navigate to a certain micro frontend, like so - 
- Imagine the checkout microservice is hosted on AWS having the URL - https://www.aws.amazon.com/onlinegamestore/checkout & the payment service hosted on Google Cloud with the URL -https://www.cloud.google.com/onlinegamestore/payment
- When the micro frontends are integrated via basic links when the user navigates from the checkout page to the payment page, the address in the browser changes from the AWS URL to the Google Cloud URL that is visible to the end-user.
- Following this basic approach, the micro frontends that need to be integrated within a specific page can be done using Iframes.
- Well, you would have already realized, this approach is not ideal. This is more like the 90s way of building websites, connecting web pages via direct links & Iframes.
- A recommended way to integrate components on the client-side is by leveraging a technology called the Web Components & frameworks such as Single SPA
- When the UI components are integrated on the server, on the user request the complete pre-built page of the website is delivered to the client from the server as opposed to sending individual micro frontends to the client and then having them clubbed there.
- This cuts down the loading time of the website on the client significantly since the browser does not have to do any sort of heavy lifting.
- Just like the client-side integration process, we have to write separate logic on the server to integrate the micro frontends that are requested by the user. 
- Open Components is an open-source micro frontends framework used for integrating micro frontends on the server. Podium is another framework that facilitates easy server-side composition of micro frontends.

35. Introduction & Types of Data
- A database is a component required to persist data. Data can be of many forms: structured, unstructured, semi-structured and user state data.
- Structured data is the type of data which conforms to a certain structure, typically stored in a database in a normalized fashion.
- Structured data is generally managed by a query language such as SQL (Structured query language).
- Unstructured data has no definite structure. It is generally the heterogeneous type of data comprising of text, image files, video, multimedia files, pdfs, Blob objects, word documents, machine-generated data etc.
- This kind of data is often encountered in data analytics. Here the data streams in from multiple sources such as IoT devices, social networks, web portals, industry sensors etc. into the analytics systems.
- We cannot just directly process unstructured data. The initial data is pretty raw, we have to make it flow through a data preparation stage which segregates it based on some business logic & then runs the analytics algorithms on it.
- Semi-structured data is a mix of structured & unstructured data. Semi-structured data is often stored in data transport formats such as XML or JSON and is handled as per the business requirements.
- The data containing the user state is the information of activity which the user performs on the website.
- For instance, when browsing through an e-commerce website, the user would browse through several product categories, change the preferences, add a few products to the reminder list for the price drops.
- All this activity is the user state. So next time, when the user logs in, he can continue from where he left off. It would not feel like that one is starting fresh & all the previous activity is lost.
- Storing user state improves the browsing experience & the conversion rate for the business.

36. Relational Database

	What Is A Relational Database?
	What Are Relationships?
	Data Consistency
	ACID Transactions
- Ensure saving data in a normalized fashion. In very simple terms, normalized data means a unique entity occurs in only one place/table, in its simplest and atomic form and is not spread throughout the database.	
- ACID – Atomicity, Consistency, Isolation, Durability.

37. NoSQL Databases - Introduction

    What Is A NoSQL Database?
    How Is A NoSQL Database Different From A Relational Database?
    Scalability
    Clustering
- NoSQL databases have no SQL, they are more like JSON-based databases built for Web 2.0
- One big limitation with SQL based relational databases is Scalability. Scaling relational databases is something which is not trivial. They have to be Sharded or Replicated to make them run smoothly on a cluster. In short, this requires careful thought and human intervention.  
- On the contrary, NoSQL databases have the ability to add new server nodes on the fly & continue the work, without any human intervention, just with a snap of the fingers.
- NoSQL databases are designed to run intelligently on clusters. And when I say intelligently, I mean with minimal human intervention.
- Today, the server nodes even have self-healing capabilities. That’s pretty smooth. The infrastructure is intelligent enough to self-recover from faults.
- Also, NoSQL databases had to sacrifice Strong consistency, ACID Transactions & much more to scale horizontally over a cluster & across the data centres.
- The data with NoSQL databases is more Eventually Consistent as opposed to being Strongly Consistent.

38. Features Of NoSQL Databases

    Pros Of NoSQL Databases
    Gentle Learning Curve
    Schemaless
    Cons Of NoSQL Databases
    Inconsistency
    No Support For ACID Transactions
    Conclusion
    Popular NoSQL Databases
- NoSQL databases are also developer-friendly.
- The learning curve is less than that of relational databases. When working with relational databases a big chunk of our time goes into learning how to design well-normalized tables, setting up relationships, trying to minimize joins and stuff.    
- There are no strict enforced schemas, work with the data as you want. You can always change stuff, spread things around. Entities have no relationships. Thus, things are flexible & you can do stuff your way.
- Not always!! This flexibility is good and bad at the same time. Being so flexible, developer-friendly, having no joins and relationships etc. makes it good.
- But it introduces a risk of entities being inconsistent at the same time. Since an entity is spread throughout the database one has to update the new values of the entity at all places.
- Failing to do so, makes the entity inconsistent. This is not a problem with relational databases since they keep the data normalized. An entity resides at one place only.
- Also, NoSQL distributed databases don’t provide ACID transactions. A few that claim to do that, don’t support them globally. They are just limited to a certain entity hierarchy or a small region where they can lock down nodes to update them.

    Note: Transactions in distributed systems come with terms and conditions applied.

- Things are comparatively simpler, as there was no stress of managing joins, relationships, n+1 query issues etc    
- Just fetch the data with its Key. You can also call it the id of the entity. This is a constant O(1) operation, which makes NoSQL Dbs really fast. NoSQL database is a lot easier than working with relationships.
- It’s alright if we need to make a few extra calls to the backend to fetch data in separate calls that doesn’t make much of a difference. We can always cache the frequently accessed data to overcome that.
- Some of the popular NoSQL databases used in the industry are MongoDB, Redis, Neo4J, Cassandra.

39. When To Pick A NoSQL Database?

    Handling A Large Number Of Read Write Operations
    Flexibility With Data Modeling
    Eventual Consistency Over Strong Consistency
    Running Data Analytics
- When there are a large number of read-write operations on your website & when dealing with a large amount of data, NoSQL databases fit best in these scenarios. Since they have the ability to add nodes on the fly, they can handle more concurrent traffic & big amount of data with minimal latency.
- The second cue is during the initial phases of development when you are not sure about the data model, the database design, things are expected to change at a rapid pace. NoSQL databases offer us more flexibility.
- It’s preferable to pick NoSQL databases when it’s OK for us to give up on Strong consistency and when we do not require transactions.
- A good example of this is a social networking website like Twitter. When a tweet of a celebrity blows up and everyone is liking and re-tweeting it from around the world. Does it matter if the count of likes goes up or down a bit for a short while?
- The celebrity would definitely not care if instead of the actual 5 million 500 likes, the system shows the like count as 5 million 250 for a short while.
- When a large application is deployed on hundreds of servers spread across the globe, the geographically distributed nodes take some time to reach a global consensus.    
- Until they reach a consensus, the value of the entity is inconsistent. The value of the entity eventually gets consistent after a short while. This is what Eventual Consistency is.
- Though the inconsistency does not mean that there is any sort of data loss. It just means that the data takes a short while to travel across the globe via the internet cables under the ocean to reach a global consensus and become consistent.
- We experience this behaviour all the time. Especially on YouTube. Often you would see a video with 10 views and 15 likes. How is this even possible?
- It’s not. The actual views are already more than the likes. It’s just the count of views is inconsistent and takes a short while to get updated. I will discuss Eventual consistency in more detail further down the course.
- NoSQL databases also fit best for data analytics use cases, where we have to deal with an influx of massive amounts of data.
- There are dedicated databases for use cases like this such as Time-Series databases, Wide-Column, Document Oriented etc.

40. Is NoSQL More Performant than SQL?

    Why Do Popular Tech Stacks Always Pick NoSQL Databases?
    Real World Case Studies
    Using Both SQL & NoSQL Database In An Application
- From a technology benchmarking standpoint, both relational and non-relational databases are equally performant.
- More than the technology, it’s how we design our systems using the technology that affects the performance.
- So, don’t get confused with all the hype. Understand your use case and then pick the technology accordingly.
- But why do the popular tech stacks always pick NoSQL databases? For instance, the MEAN (MongoDB, ExpressJS, AngularJS/ReactJS, NodeJs) stack.
- Well, most of the applications online have common use cases. And these tech stacks have them covered. There are also commercial reasons behind this.
- Coming back to the performance, it entirely depends on the application & the database design. If we are using more Joins with SQL. The response will inevitably take more time.
- If we remove all the relationships and joins, SQL becomes just like NoSQL.
- Facebook uses MySQL for storing its social graph of millions of users. Though it did have to change the DB engine and make some tweaks but MySQL fits best for its use case.

 Note: A well-designed SQL data store will always be more performant than a not so well-designed NoSQL store.
- As a matter of fact, all the large-scale online services use a mix of both to implement their systems and achieve the desired behaviour.
- The term for leveraging the power of multiple databases is called Polyglot Persistence.  

41. Polyglot Persistence

    What Is Polyglot Persistence?
    Real World Use Case
    Relational Database
    Key Value Store
    Wide Column Database
    ACID Transactions & Strong Consistency
    Graph Database
    Document Oriented Store
    Downside Of This Approach
- Polyglot persistence means using several different persistence technologies to fulfil different persistence requirements in an application.
- Let’s say we are writing a social network like Facebook.
- To store relationships like persisting friends of a user, friends of friends, what rock band they like, what food preferences they have in common etc. we would pick a relational database like MySQL.
- For low latency access of all the frequently accessed data, we will implement a cache using a Key-value store like Redis or Memcache.
- We can use the same Key-value data store to store user sessions.
- Now our app is already a big hit, it has got pretty popular and we have millions of active users.
- To understand user behaviour, we need to set up an analytics system to run analytics on the data generated by the users. We can do this using a wide-column database like Cassandra or HBase.
- The popularity of our application just doesn’t seem to stop, it’s soaring. Now businesses want to run ads on our portal. For this, we need to set up a payments system.
- Again, we would pick a relational database to implement ACID transactions & ensure Strong consistency.
- Now to enhance the user experience of our application we have to start recommending content to the users to keep them engaged. A Graph database would fit best to implement a recommendation system.
- Alright, by now, our application has multiple features, & everyone loves it. How cool it would be if a user can run a search for other users, business pages and stuff on our portal & could connect with them?
- To implement Document Oriented Store, we can use an open-source document-oriented datastore like ElasticSearch. The product is pretty popular in the industry for implementing a scalable search feature on websites. We can persist all the search-related data in the elastic store.
- One downside of this approach is increased complexity in making all these different technologies work together.
- A lot of effort goes into building, managing and monitoring polyglot persistence systems. What if there was something simpler? That would save us the pain of putting together everything ourselves. Well, there is.

42. Multi-Model Databases
- Until now the databases supported only one data model, it could either be a relational database, a graph database or any other database with a certain specific data model.
- But with the advent of multi-model databases, we have the ability to use different data models in a single database system.
- Multi-model databases support multiple data models like Graph, Document-Oriented, Relational etc. as opposed to supporting only one data model.
- They also avert the need for managing multiple persistence technologies in a single service. They reduce the operational complexity by notches. With multi-model databases, we can leverage different data models via a single API.
- Some of the popular multi-model databases are Arango DB, Cosmos DB, Orient DB, Couchbase etc.

43. Eventual Consistency
- Eventual consistency is a consistency model which enables the data store to be highly available. It is also known as optimistic replication & is key to distributed systems.
- Think of a popular microblogging site deployed across the world in different geographical regions like Asia, America, Europe. Moreover, each geographical region has multiple data centre zones: North, East, West, South. Furthermore, each of the zones has multiple clusters which have multiple server nodes running.
- So, we have many datastore nodes spread across the world which the micro-blogging site uses for persisting data.
- Since there are so many nodes running, there is no single point of failure. The data store service is highly available. Even if a few nodes go down the persistence service as a whole is still up.
- Alright, now let’s say a celebrity makes a post on the website which everybody starts liking around the world.
- At a point in time, a user in Japan likes the post which increases the “Like” count of the post from say 100 to 101. At the same point in time, a user in America, in a different geographical zone clicks on the post and he sees the “Like” count as 100, not 101.
- This happens because the new updated value of the Post “Like” counter needs some time to move from Japan to America and update the server nodes running there.
- Though the value of the counter at that point in time was 101, the user in America sees the old inconsistent value.
- But when he refreshes his web page after a few seconds the “Like” counter value shows as 101. So, the data was initially inconsistent but eventually got consistent across the server nodes deployed around the world. This is what eventual consistency is.
- Let’s take it one step further, what if at the same point in time both the users in Japan and America Like the post, and a user in another geographic zone say Europe accesses the post.
- All the nodes in different geographic zones have different post values. And they will take some time to reach a consensus.
- The upside of eventual consistency is that the system can add new nodes on the fly without the need to block any of them, the nodes are available to the end-users to make an update at all times.
- Millions of users across the world can update the values at the same time without having to wait for the system to reach a common final value across all nodes before they make an update. This feature enables the system to be highly available.
- Eventual consistency is suitable for use cases where the accuracy of values doesn’t matter much like in the above-discussed use case.
- Other use cases of eventual consistency can be when keeping the count of users watching a Live video stream online. When dealing with massive amounts of analytics data. A couple of counts up and down won’t matter much.
- But there are use cases where the data has to be laser accurate like in banking, stock markets. We just cannot have our systems to be Eventually Consistent, we need Strong Consistency

44. Strong Consistency
- Strong Consistency simply means the data has to be strongly consistent at all times. All the server nodes across the world should contain the same value of an entity at any point in time. And the only way to implement this behaviour is by locking down the nodes when being updated.
- To ensure Strong Consistency in the system, when the user in Japan likes the post, all the nodes across different geographical zones have to be locked down to prevent any concurrent updates. This means at one point in time, only one user can update the post “Like” counter value.
- So, once the user in Japan updates the “Like” counter from 100 to 101. The value gets replicated globally across all nodes. Once all the nodes reach a consensus, the locks get lifted. Now, other users can Like the post. If the nodes take a while to reach a consensus, they have to wait until then.
- Well, this is surely not desired in case of social applications. But think of a stock market application where the users are seeing different prices of the same stock at one point in time and updating it concurrently. This would create chaos.
- Therefore, to avoid this confusion we need our systems to be Strongly Consistent. The nodes have to be locked down for updates.
- Queuing all the requests is one good way of making a system Strongly Consistent. We have a theorem called the CAP theorem which is key to implementing these consistency models.
- Strong Consistency model hits the capability of the system to be Highly Available. The system while being updated by one user does not allow other users to perform concurrent updates. This is how strongly consistent ACID transactions are implemented.
- Distributed systems like NoSQL databases which scale horizontally on the fly don’t support ACID transactions globally & this is due to their design. The whole reason for the development of NoSQL tech is the ability to be Highly Available and Scalable. If we have to lock down the nodes every time, it becomes just like SQL.
- So, NoSQL databases don’t support ACID transactions and those that claim to, have terms and conditions applied to them.
- Generally, the transaction support is limited to a geographic zone or an entity hierarchy. Developers of the tech make sure that all the Strongly consistency entity nodes reside in the same geographic zone to make the ACID transactions possible.

45. CAP Theorem
- CAP stands for Consistency, Availability, Partition Tolerance. Partition Tolerance means Fault Tolerance. The system is tolerant of failures or partitions. It keeps working even if a few nodes go down.
- CAP theorem simply states that in case of a network failure, when a few of the nodes of the system are down, we have to make a choice between Availability & Consistency
- If we pick Availability that means when a few nodes go down, the other nodes are available to the users for making updates. In this situation, the system is inconsistent as the nodes which are down don’t get updated with the new data. At the point in time when they come back online, if a user fetches the data from them, they’ll return the old values they had when they went down.
- If we pick Consistency, in that scenario, we have to lock down all the nodes for further writes until the nodes which have gone down come back online. This would ensure the Strong consistency of the system as all the nodes will have the same entity values.
- Picking between Availability and Consistency largely depends on our use case and the business requirements. We have been through this in great detail. Also, the limitation of picking one out of the two is due to the design of the distributed systems. We can’t have both Availability and Consistency at the same time.
- Nodes, spread around the globe, will take some time to reach a consensus. It’s impossible to have zero-latency unless we transit data faster than or at the speed of time.
- It’s the speed of time, not the speed of light. Speed of light does not have zero latency, it does take some time to travel from one point to the other.

46. Types of Databases
- There are quite a number of different types of databases available to the application developers, catering to specific use cases.
    Document-Oriented database
    Key-value datastore
    Wide-column database
    Relational database
    Graph database
    Time-Series database
    Databases dedicated to mobile apps    

47. Document Oriented Database

    What Is A Document Oriented Database?
    Popular Document Oriented Databases
    When Do I Pick A Document Oriented Data Store for My Project?
    Real Life Implementations
- Document Oriented databases are the main types of NoSQL databases. They store data in a document-oriented model in independent documents. The data is generally semi-structured & stored in a JSON-like format.
- The data model is similar to the data model of our application code, so it’s easier to store and query data for developers.
- Document oriented stores are suitable for Agile software development methodology as it’s easier to change things with evolving demands when working with them.
- Some of the popular document-oriented stores used in the industry are MongoDB, CouchDB, OrientDB, Google Cloud Datastore, Amazon Document DB
- If you are working with semi-structured data, need a flexible schema which would change often. You ain’t sure about the database schema when you start writing the app. There is a possibility that things might change over time. You are in need of something flexible which you could change over time with minimum fuss. Pick a Document-Oriented data store.
- Typical use cases of Document oriented databases are the following:
    Real-time feeds
    Live sports apps
    Writing product catalogues
    Inventory management
    Storing user comments
    Web-based multiplayer games
- Being in the family of NoSQL databases these provide horizontal scalability, performant read-writes as they cater to CRUD - Create Read Update Delete use cases. Where there isn’t much relational logic involved & all we need is just quick persistence & retrieval of data.
- Here are some of the good real-life implementations of the tech below -
    SEGA uses Mongo-DB to improve the experience for millions of mobile gamers
    Coinbase scaled from 15k requests per min to 1.2 million requests per minute with MongoDB

48. Graph Database

    What Is A Graph Database?
    Features Of A Graph Database
    When Do I Pick A Graph Database?
    Real Life Implementations

- Graph databases are also a part of the NoSQL database family. They store data in nodes/vertices and edges in the form of relationships.
- Each Node in a graph database represents an entity. It can be a person, a place, a business etc. And the Edge represents the relationship between the entities.
- But, why use a graph database to store relationships when we already have SQL based relational databases available?
- Primarily, two reasons. The first is visualization. Think of that pinned board in the thriller detective movies where the pins are pinned on a board over several images connected via threads. It does help in visualizing how the entities are related & how things fit together. Right?
- The second reason is the low latency. In graph databases, the relationships are stored a bit differently from how the relational databases store relationships.
- Graph databases are faster as the relationships in them are not calculated at the query time, as it happens with the help of joins in the relational databases. Rather the relationships here are persisted in the data store in the form of edges and we just have to fetch them. No need to run any sort of computation at the query time.
- A good real-life example of an application which would fit a graph database is Google Maps. Nodes represent the cities and the Edges represent the connection between them.
- Now, if I have to look for roads between different cities, I don’t need joins to figure out the relationship between the cities when I run the query. I just need to fetch the edges which are already stored in the database.
- Ideal use cases of graph databases are building social, knowledge, network graphs. Writing AI-based apps, recommendation engines, fraud analysis app, storing genetic data etc.
- Graph databases help us visualize our data with minimum latency. A popular graph database used in the industry is Neo4J.
- Walmart shows product recommendations to its customers in real-time using Neo4J graph database
- NASA uses Neo4J to store “lessons learned” data from their previous missions to educate the scientists & engineers.

49. Key Value Database

    What Is A Key Value Database?
    Features Of A Key Value Database
    Popular Key Value Databases
    When Do I Pick A Key Value Database?
    Real Life Implementations

- Key-value databases also are a part of the NoSQL family. These databases use a simple key-value method to store and quickly fetch the data with minimum latency.
- A primary use case of a Key-value database is to implement caching in applications due to the minimum latency they ensure.
- The Key serves as a unique identifier and has a value associated with it. The value can be as simple as a block of text & can be as complex as an object graph.
- The data in Key-value databases can be fetched in constant time O(1), there is no query language required to fetch the data. It’s just a simple no-brainer fetch operation. This ensures minimum latency.
- Some of the popular key-value data stores used in the industry are Redis, Hazelcast, Riak, Voldemort & Memcache.
- If you have a use case where you need to fetch data real fast with minimum fuss & backend processing then you should pick a key-value data store.
- Key-value stores are pretty efficient in pulling off scenarios where super-fast data fetch is the order of the day.
- Typical use cases of a key value database are the following:
    Caching
    Persisting user state
    Persisting user sessions
    Managing real-time data
    Implementing queues
    Creating leaderboards in online games & web apps
    Implementing a pub-sub system
- Some of the real-life implementations of the tech are -
    Inovonics uses Redis to drive real-time analytics on millions of sensor data
    Microsoft uses Redis to handle the traffic spike on its platforms
    Google Cloud uses Memcache to implement caching on their cloud platform

50. Time Series Database

    What Is A Time Series Database?
    What Is Time Series Data?
    Why Store Time Series Data?
    Popular Time Series Databases
    When To Pick A Time Series Database?
    Real Life Implementations

- Time-Series databases are optimized for tracking & persisting time series data.
- It is the data containing data points associated with the occurrence of an event with respect to time. These data points are tracked, monitored and then finally aggregated based on certain business logic.
- Time-Series data is generally ingested from IoT devices, self-driving vehicles, industry sensors, social networks, stock market financial data etc.
- Studying data, streaming-in from applications helps us track the behaviour of the system. It helps us study user patterns, anomalies & how things change over time.
- Time-series data is primarily used for running analytics, deducing conclusions and making future business decisions looking at the results of the analytics. Running analytics also helps the product evolve continually.
- General databases are not built to handle time-series data. With the advent of IoT, these databases are getting pretty popular and are being adopted by the big guns in the industry.
- Some of the popular time-series databases used in the industry are Influx DB, Timescale DB, Prometheus etc.
- If you have a use case where you need to manage data in real-time & continually over a long period of time, then a time-series database is what you need.
-As we know that the time-series databases are built to deal with data, streaming in real-time. The typical use cases of it are fetching data from IoT devices. Managing data for running analytics & monitoring. Writing an autonomous trading platform which deals with changing stock prices in real-time etc.
- Here are some of the real-life implementations of the tech -
    IBM uses Influx DB to run analytics for real-time cognitive fraud detection
    Spiio uses Influx DB to remotely monitor vertical lining green walls & plant installations.

51. Wide-Column Database

    What Is A Wide Column Database?
    Popular Wide Column Databases
    When To Pick a Wide Column Database?
    Real-Life Implementations

- Wide-column databases belong to the NoSQL family of databases, primarily used to handle massive amounts of data, technically called the Big Data.    
- Wide-column databases are perfect for analytical use cases. They have a high performance and a scalable architecture.
- Also, known as column-oriented databases wide-column databases store data in a record with a dynamic number of columns. A record can hold billions of columns.
- Some of the popular wide column databases are Cassandra, HBase, Google BigTable, Scylla DB etc.
- If you have a use case where you need to grapple with Big data, to ingest it or to run analytics on it, then a wide-column database is a good fit for this scenario.
- Wide-column databases are built to manage big data ensuring scalability, performance & high availability at the same time.
- Some of the real-life implementations of the tech are -
    Netflix uses Cassandra as the backend database for the streaming service\
    Adobe uses HBase for processing large amounts of data

52. Cashing Introduction

    What Is Caching?
    Caching Dynamic Data
    Caching Static Data

- Caching is key to the performance of any kind of application. It ensures low latency and high throughput. An application with caching will certainly do better than an application without caching, simply because it returns the response in less time as opposed to the application without a cache implemented.
- Implementing caching in a web application simply means copying frequently accessed data from the database which is disk-based hardware and storing it in RAM Random Access Memory hardware.
- RAM-based hardware provides faster access than the disk-based hardware. As I said earlier it ensures low latency and high throughput. Throughput means the number of network calls i.e. request-response between the client and the server within a stipulated time.
- RAM-based hardware is capable of handling more requests than the disk-based hardware, on which the databases run.
- With caching we can cache both the static data and the dynamic data. Dynamic data is the data which changes more often, it has an expiry time or a TTL “Time To Live”. After the TTL ends, the data is purged from the cache and the newly updated data is stored in it. This process is known as Cache Invalidation.
- Static data consists of images, font files, CSS & other similar files. This is the kind of data which doesn’t change often & can easily be cached on the client-side in the browser or their local memory. Also, on the CDNs the Content Delivery Networks.
- Caching also helps applications maintain their expected behaviour during network interruptions.    









    



















































