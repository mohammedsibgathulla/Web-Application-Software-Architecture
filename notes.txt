1. Single Tier Applications
- A single-tier application is an application where the user interface, backend business logic & the database all reside in the same machine.
- The main upside of single-tier applications is they have no network latency since every component is located on the same machine. This adds up to the performance of the software.

2. Two Tier Application 
- A Two-tier application involves a client and a server. The client would contain the user interface & the business logic in one machine. And the backend server would be the database running on a different machine. The database server is hosted by the business & has control over it.

3. Three Tier applications
- In a three-tier application, the user interface, application logic & the database all lie on different machines & thus have different tiers. They are physically separated.

4. N Tier Applications
- An N-tier application is an application which has more than three components involved. For example 
  - Cache
  - Message queues for asynchronous behaviour
  - Load balancers
  - Search servers for searching through massive amounts of data
  - Components involved in processing massive amounts of data
  - Components running heterogeneous tech commonly known as web services etc.
- Note: There is another name for n-tier apps, the “distributed applications”. But, I think it’s not safe to use the word “distributed” yet, as the term distributed brings along a lot of complex stuff with it. It would rather confuse us than help. Though I will discuss the distributed architecture in this course, for now, we will just stick with the term N-tier applications.
- Why the need for so many tiers? 
- Two software design principles that are key to explaining this are the Single Responsibility Principle & the Separation of Concerns.
- Single Responsibility Principle simply means giving one, just one responsibility to a component & letting it execute it with perfection. Be it saving data, running the application logic or ensuring the delivery of the messages throughout the system.
- Separation of concerns kind of means the same thing, be concerned about your work only & stop worrying about the rest of the stuff. Keeping the components separate makes them reusable. Different services can use the same database, the messaging server or any component as long as they are not tightly coupled with each other.
- Note: Don’t confuse tiers with the layers of the application. Some prefer to use them interchangeably. But in the industry layers of an application typically means the user interface layer, business layer, service layer, or the data access layer.

5. What Is Web Architecture?
- Web architecture involves multiple components like database, message queue, cache, user interface & all running in conjunction with each other to form an online service.

6. Client Server Architecture
- The architecture works on a request-response model. The client sends the request to the server for information & the server responds with it.
- A very small percent of the business websites and applications use the peer to peer architecture, which is different from the client-server.
- Every website you browse, be it a Wordpress blog or a web application like Facebook, Twitter or your banking app is built on the client-server architecture.

7. Client
- The client holds our user interface. The user interface is the presentation part of the application. It’s written in Html, JavaScript, CSS and is responsible for the look & feel of the application.
- The user interface runs on the client. The client can be a mobile app, a desktop or a tablet like an iPad. It can also be a web-based console, running commands to interact with the backend server.

8. Types of Client
- Thin Client & Thick Client
- Thin Client is the client which holds just the user interface of the application. It has no business logic of any sort. For every action, the client sends a request to the backend server. Just like in a three-tier application.
- On the contrary, the thick client holds all or some part of the business logic. These are the two-tier applications. We’ve already gone through this if you remember.

9. Server
- The primary task of a web server is to receive the requests from the client & provide the response after executing the business logic based on the request parameters received from the client.
- Every service, running online, needs a server to run. Servers running web applications are commonly known as the application servers.
- Besides the application servers, there are other kinds of servers too with specific tasks assigned to them such as the:	
    Proxy server
    Mail server
    File server
    Virtual server
- All the components of a web application need a server to run. Be it a database, a message queue, a cache or any other component. In modern application development, even the user interface is hosted separately on a dedicated server.

10. Communication Between the Client & the Server
- The client & the server have a request-response model. The client sends the request & the server responds with the data.
- The entire communication happens over the HTTP protocol. It is the protocol for data exchange over the World Wide Web. HTTP protocol is a request-response protocol that defines how information is transmitted across the web.
- It’s a stateless protocol, every process over HTTP is executed independently & has no knowledge of previous processes.
- Speaking from the context of modern N-tier web applications, every client has to hit a REST end-point to fetch the data from the backend.
- The backend application code has a REST-API implemented which acts as an interface to the outside world requests. Every request be it from the client written by the business or the third-party developers which consume our data have to hit the REST-endpoints to fetch the data.

11. HTTP Push & Pull - Introduction
- For every response, there has to be a request first. The client sends the request & the server responds with the data. This is the default mode of HTTP communication, called the HTTP PULL mechanism.    
- In HTTP PUSH based mechanism the client sends the request for particular information to the server, just for the first time, & after that the server keeps pushing the new updates to the client whenever they are available. The client doesn’t have to worry about sending requests to the server, for data, every now & then. This saves a lot of network bandwidth & cuts down the load on the server by notches. This is also known as a Callback. Client phones the server for information. The server responds, Hey!! I don’t have the information right now but I’ll call you back whenever it is available.
- There are multiple technologies involved in the HTTP Push based mechanism such as:	
    Ajax Long polling
    Web Sockets
    HTML5 Event Source
    Message Queues
    Streaming over HTTP

12. HTTP Pull - Polling with Ajax
- There are two ways of pulling/fetching data from the server. The first is sending an HTTP GET request to the server manually by triggering an event, like by clicking a button or any other element on the web page.
- The other is fetching data dynamically at regular intervals by using AJAX without any human intervention. AJAX is commonly used with the Jquery framework to implement the asynchronous behaviour on the UI. 
- This dynamic technique of requesting information from the server after regular intervals is known as Polling.

13. HTTP Push
- The above mechanism has the following
    Time To Live (TTL)
    Persistent Connection
    Heartbeat Interceptors
    Resource Intensive
- In the regular client-server communication, which is HTTP PULL, there is a Time to Live (TTL) for every request. It could be 30 secs to 60 secs, varies from browser to browser.
- If the client doesn’t receive a response from the server within the TTL, the browser kills the connection & the client has to re-send the request hoping it would receive the data from the server before the TTL ends this time.
- Open connections consume resources & there is a limit to the number of open connections a server can handle at one point in time. If the connections don’t close & new ones are being introduced, over time, the server will run out of memory. Hence, the TTL is used in client-server communication.
- But what if we are certain that the response will take more time than the TTL set by the browser?. In this case, we need a Persistent Connection between the client and the server. 
- A persistent connection is a network connection between the client & the server that remains open for further requests & the responses, as opposed to being closed after a single communication. It facilitates HTTP Push-based communication between the client and the server.    
- Now you might be wondering how is a persistent connection possible if the browser kills the open connections to the server every X seconds?. The connection between the client and the server stays open with the help of Heartbeat Interceptors.
- These are just blank request responses between the client and the server to prevent the browser from killing the connection.
- Persistent connections consume a lot of resources in comparison to the HTTP Pull behaviour. But there are use cases where establishing a persistent connection is vital to the feature of an application.For instance, a browser-based multiplayer game has a pretty large amount of request-response activity within a certain time in comparison to a regular web application.
- It would be apt to establish a persistent connection between the client and the server from a user experience standpoint.
- Long opened connections can be implemented by multiple techniques such as Ajax Long Polling, Web Sockets, Server-Sent Events etc.

14. HTTP Push-Based Technologies
- The below are the HTTP Push-Based technologies  
    Web Sockets
    AJAX – Long Polling
    HTML5 Event Source API & Server Sent Events
    Streaming Over HTTP
- A Web Socket connection is ideally preferred when we need a persistent bi-directional low latency data flow from the client to server & back.
- Typical use-cases of these are messaging, chat applications, real-time social streams & browser-based massive multiplayer games which have quite a number of read writes in comparison to a regular web app.
- With Web Sockets, we can keep the client-server connection open as long as we want. Have bi-directional data? Go ahead with Web Sockets. One more thing, Web Sockets tech doesn’t work over HTTP. It runs over TCP. The server & the client should both support web sockets or else it won’t work.
- Long Polling lies somewhere between Ajax & Web Sockets. In this technique instead of immediately returning the response, the server holds the response until it finds an update to be sent to the client. The connection in long polling stays open a bit longer in comparison to polling. The server doesn’t return an empty response. If the connection breaks, the client has to re-establish the connection to the server. The upside of using this technique is that there are quite a smaller number of requests sent from the client to the server, in comparison to the regular polling mechanism. This reduces a lot of network bandwidth consumption. Long polling can be used in simple asynchronous data fetch use cases when you do not want to poll the server every now & then.
- The Server-Sent Events implementation takes a bit of a different approach. Instead of the client polling for data, the server automatically pushes the data to the client whenever the updates are available. The incoming messages from the server are treated as Events. Via this approach, the servers can initiate data transmission towards the client once the client has established the connection with an initial request.
- This helps in getting rid of a huge number of blank request-response cycles cutting down the bandwidth consumption by notches.
- To implement server-sent events, the backend language should support the technology & on the UI HTML5 Event source API is used to receive the data in-coming from the backend.
- An important thing to note here is that once the client establishes a connection with the server, the data flow is in one direction only, that is from the server to the client.
- SSE is ideal for scenarios such as a real-time feed like that of Twitter, displaying stock quotes on the UI, real-time notifications etc.
- Streaming Over HTTP is ideal for cases where we need to stream large data over HTTP by breaking it into smaller chunks. This is possible with HTML5 & a JavaScript Stream API.
- The technique is primarily used for streaming multimedia content, like large images, videos etc, over HTTP. Due to this, we can watch a partially downloaded video as it continues to download, by playing the downloaded chunks on the client. This helps them figure when the stream begins & ends over an HTTP request-response model.

15. Client-Side Vs Server-Side Rendering
- When a user requests a web page from the server & the browser receives the response. It has to render the response on the window in the form of an HTML page. For this, the browser has several components, such as the:
    Browser engine
    Rendering engine
    JavaScript interpreter
    Networking & the UI backend
    Data storage etc.
- The rendering engine constructs the DOM tree, renders & paints the construction. And naturally, all this activity needs a bit of time.
- To avoid all this rendering time on the client, developers often render the UI on the server, generate HTML there & directly send the HTML page to the UI.
- This technique is known as the Server-side rendering. It ensures faster rendering of the UI, averting the UI loading time in the browser window since the page is already created & the browser doesn’t have to do much assembling & rendering work.
- The server-side rendering approach is perfect for delivering static content, such as WordPress blogs. It’s also good for SEO as the crawlers can easily read the generated content.
- However, the modern websites are highly dependent on Ajax. In such websites, content for a particular module or a section of a page has to be fetched & rendered on the fly. Therefore, server-side rendering doesn’t help much. For every Ajax-request, instead of sending just the required content to the client, the approach generates the entire page on the server. This process consumes unnecessary bandwidth & also fails to provide a smooth user experience.
- A big downside to this is once the number of concurrent users on the website rises, it puts an unnecessary load on the server.
- Client-side rendering works best for modern dynamic Ajax-based websites. Though we can leverage a hybrid approach, to get the most out of both techniques. We can use server-side rendering for the home page & for the other static content on our website & use client-side rendering for the dynamic pages.

16. What is Scalability?
- Scalability means the ability of the application to handle & withstand increased workload without sacrificing the latency.
- Latency is the amount of time a system takes to respond to a user request. Let’s say you send a request to an app to fetch an image & the system takes 2 seconds to respond to your request. The latency of the system is 2 seconds.
- Minimum latency is what efficient software systems strive for. No matter how much the traffic load on a system builds up, the latency should not go up. This is what scalability is.
- If the latency remains the same, we can say yeah, the application scaled well with the increased load & is highly scalable.
- Latency is measured as the time difference between the action that the user takes on the website, it can be an event like the click of a button, & the system response in reaction to that event.
- This latency is generally divided into two parts:
    Network Latency
    Application Latency
- Network Latency is the amount of time that the network takes for sending a data packet from point A to point B. The network should be efficient enough to handle the increased traffic load on the website. To cut down the network latency, businesses use CDN & try to deploy their servers across the globe as close to the end-user as possible.
- Application Latency is the amount of time the application takes to process a user request. There are more than a few ways to cut down the application latency. The first step is to run stress & load tests on the application & scan for the bottlenecks that slow down the system as a whole. I’ve talked more about it in the upcoming lesson.
- We can realize the importance of low latency by the fact that Huawei & Hibernia Atlantic in the year 2011 started laying a fibre-optic link cable across the Atlantic Ocean between London & Newyork, that was estimated having a cost of approx. $300M, just to save traders 6 milliseconds of latency.
   











