1. Single Tier Applications
- A single-tier application is an application where the user interface, backend business logic & the database all reside in the same machine.
- The main upside of single-tier applications is they have no network latency since every component is located on the same machine. This adds up to the performance of the software.

2. Two Tier Application 
- A Two-tier application involves a client and a server. The client would contain the user interface & the business logic in one machine. And the backend server would be the database running on a different machine. The database server is hosted by the business & has control over it.

3. Three Tier applications
- In a three-tier application, the user interface, application logic & the database all lie on different machines & thus have different tiers. They are physically separated.

4. N Tier Applications
- An N-tier application is an application which has more than three components involved. For example 
  - Cache
  - Message queues for asynchronous behaviour
  - Load balancers
  - Search servers for searching through massive amounts of data
  - Components involved in processing massive amounts of data
  - Components running heterogeneous tech commonly known as web services etc.
- Note: There is another name for n-tier apps, the “distributed applications”. But, I think it’s not safe to use the word “distributed” yet, as the term distributed brings along a lot of complex stuff with it. It would rather confuse us than help. Though I will discuss the distributed architecture in this course, for now, we will just stick with the term N-tier applications.
- Why the need for so many tiers? 
- Two software design principles that are key to explaining this are the Single Responsibility Principle & the Separation of Concerns.
- Single Responsibility Principle simply means giving one, just one responsibility to a component & letting it execute it with perfection. Be it saving data, running the application logic or ensuring the delivery of the messages throughout the system.
- Separation of concerns kind of means the same thing, be concerned about your work only & stop worrying about the rest of the stuff. Keeping the components separate makes them reusable. Different services can use the same database, the messaging server or any component as long as they are not tightly coupled with each other.
- Note: Don’t confuse tiers with the layers of the application. Some prefer to use them interchangeably. But in the industry layers of an application typically means the user interface layer, business layer, service layer, or the data access layer.

5. What Is Web Architecture?
- Web architecture involves multiple components like database, message queue, cache, user interface & all running in conjunction with each other to form an online service.

6. Client Server Architecture
- The architecture works on a request-response model. The client sends the request to the server for information & the server responds with it.
- A very small percent of the business websites and applications use the peer to peer architecture, which is different from the client-server.
- Every website you browse, be it a Wordpress blog or a web application like Facebook, Twitter or your banking app is built on the client-server architecture.

7. Client
- The client holds our user interface. The user interface is the presentation part of the application. It’s written in Html, JavaScript, CSS and is responsible for the look & feel of the application.
- The user interface runs on the client. The client can be a mobile app, a desktop or a tablet like an iPad. It can also be a web-based console, running commands to interact with the backend server.

8. Types of Client
- Thin Client & Thick Client
- Thin Client is the client which holds just the user interface of the application. It has no business logic of any sort. For every action, the client sends a request to the backend server. Just like in a three-tier application.
- On the contrary, the thick client holds all or some part of the business logic. These are the two-tier applications. We’ve already gone through this if you remember.

9. Server
- The primary task of a web server is to receive the requests from the client & provide the response after executing the business logic based on the request parameters received from the client.
- Every service, running online, needs a server to run. Servers running web applications are commonly known as the application servers.
- Besides the application servers, there are other kinds of servers too with specific tasks assigned to them such as the:	
    Proxy server
    Mail server
    File server
    Virtual server
- All the components of a web application need a server to run. Be it a database, a message queue, a cache or any other component. In modern application development, even the user interface is hosted separately on a dedicated server.

10. Communication Between the Client & the Server
- The client & the server have a request-response model. The client sends the request & the server responds with the data.
- The entire communication happens over the HTTP protocol. It is the protocol for data exchange over the World Wide Web. HTTP protocol is a request-response protocol that defines how information is transmitted across the web.
- It’s a stateless protocol, every process over HTTP is executed independently & has no knowledge of previous processes.
- Speaking from the context of modern N-tier web applications, every client has to hit a REST end-point to fetch the data from the backend.
- The backend application code has a REST-API implemented which acts as an interface to the outside world requests. Every request be it from the client written by the business or the third-party developers which consume our data have to hit the REST-endpoints to fetch the data.

11. HTTP Push & Pull - Introduction
- For every response, there has to be a request first. The client sends the request & the server responds with the data. This is the default mode of HTTP communication, called the HTTP PULL mechanism.    
- In HTTP PUSH based mechanism the client sends the request for particular information to the server, just for the first time, & after that the server keeps pushing the new updates to the client whenever they are available. The client doesn’t have to worry about sending requests to the server, for data, every now & then. This saves a lot of network bandwidth & cuts down the load on the server by notches. This is also known as a Callback. Client phones the server for information. The server responds, Hey!! I don’t have the information right now but I’ll call you back whenever it is available.
- There are multiple technologies involved in the HTTP Push based mechanism such as:	
    Ajax Long polling
    Web Sockets
    HTML5 Event Source
    Message Queues
    Streaming over HTTP

12. HTTP Pull - Polling with Ajax
- There are two ways of pulling/fetching data from the server. The first is sending an HTTP GET request to the server manually by triggering an event, like by clicking a button or any other element on the web page.
- The other is fetching data dynamically at regular intervals by using AJAX without any human intervention. AJAX is commonly used with the Jquery framework to implement the asynchronous behaviour on the UI. 
- This dynamic technique of requesting information from the server after regular intervals is known as Polling.

13. HTTP Push
- The above mechanism has the following
    Time To Live (TTL)
    Persistent Connection
    Heartbeat Interceptors
    Resource Intensive
- In the regular client-server communication, which is HTTP PULL, there is a Time to Live (TTL) for every request. It could be 30 secs to 60 secs, varies from browser to browser.
- If the client doesn’t receive a response from the server within the TTL, the browser kills the connection & the client has to re-send the request hoping it would receive the data from the server before the TTL ends this time.
- Open connections consume resources & there is a limit to the number of open connections a server can handle at one point in time. If the connections don’t close & new ones are being introduced, over time, the server will run out of memory. Hence, the TTL is used in client-server communication.
- But what if we are certain that the response will take more time than the TTL set by the browser?. In this case, we need a Persistent Connection between the client and the server. 
- A persistent connection is a network connection between the client & the server that remains open for further requests & the responses, as opposed to being closed after a single communication. It facilitates HTTP Push-based communication between the client and the server.    
- Now you might be wondering how is a persistent connection possible if the browser kills the open connections to the server every X seconds?. The connection between the client and the server stays open with the help of Heartbeat Interceptors.
- These are just blank request responses between the client and the server to prevent the browser from killing the connection.
- Persistent connections consume a lot of resources in comparison to the HTTP Pull behaviour. But there are use cases where establishing a persistent connection is vital to the feature of an application.For instance, a browser-based multiplayer game has a pretty large amount of request-response activity within a certain time in comparison to a regular web application.
- It would be apt to establish a persistent connection between the client and the server from a user experience standpoint.
- Long opened connections can be implemented by multiple techniques such as Ajax Long Polling, Web Sockets, Server-Sent Events etc.

14. HTTP Push-Based Technologies
- The below are the HTTP Push-Based technologies  
    Web Sockets
    AJAX – Long Polling
    HTML5 Event Source API & Server Sent Events
    Streaming Over HTTP
- A Web Socket connection is ideally preferred when we need a persistent bi-directional low latency data flow from the client to server & back.
- Typical use-cases of these are messaging, chat applications, real-time social streams & browser-based massive multiplayer games which have quite a number of read writes in comparison to a regular web app.
- With Web Sockets, we can keep the client-server connection open as long as we want. Have bi-directional data? Go ahead with Web Sockets. One more thing, Web Sockets tech doesn’t work over HTTP. It runs over TCP. The server & the client should both support web sockets or else it won’t work.
- Long Polling lies somewhere between Ajax & Web Sockets. In this technique instead of immediately returning the response, the server holds the response until it finds an update to be sent to the client. The connection in long polling stays open a bit longer in comparison to polling. The server doesn’t return an empty response. If the connection breaks, the client has to re-establish the connection to the server. The upside of using this technique is that there are quite a smaller number of requests sent from the client to the server, in comparison to the regular polling mechanism. This reduces a lot of network bandwidth consumption. Long polling can be used in simple asynchronous data fetch use cases when you do not want to poll the server every now & then.
- The Server-Sent Events implementation takes a bit of a different approach. Instead of the client polling for data, the server automatically pushes the data to the client whenever the updates are available. The incoming messages from the server are treated as Events. Via this approach, the servers can initiate data transmission towards the client once the client has established the connection with an initial request.
- This helps in getting rid of a huge number of blank request-response cycles cutting down the bandwidth consumption by notches.
- To implement server-sent events, the backend language should support the technology & on the UI HTML5 Event source API is used to receive the data in-coming from the backend.
- An important thing to note here is that once the client establishes a connection with the server, the data flow is in one direction only, that is from the server to the client.
- SSE is ideal for scenarios such as a real-time feed like that of Twitter, displaying stock quotes on the UI, real-time notifications etc.
- Streaming Over HTTP is ideal for cases where we need to stream large data over HTTP by breaking it into smaller chunks. This is possible with HTML5 & a JavaScript Stream API.
- The technique is primarily used for streaming multimedia content, like large images, videos etc, over HTTP. Due to this, we can watch a partially downloaded video as it continues to download, by playing the downloaded chunks on the client. This helps them figure when the stream begins & ends over an HTTP request-response model.

15. Client-Side Vs Server-Side Rendering
- When a user requests a web page from the server & the browser receives the response. It has to render the response on the window in the form of an HTML page. For this, the browser has several components, such as the:
    Browser engine
    Rendering engine
    JavaScript interpreter
    Networking & the UI backend
    Data storage etc.
- The rendering engine constructs the DOM tree, renders & paints the construction. And naturally, all this activity needs a bit of time.
- To avoid all this rendering time on the client, developers often render the UI on the server, generate HTML there & directly send the HTML page to the UI.
- This technique is known as the Server-side rendering. It ensures faster rendering of the UI, averting the UI loading time in the browser window since the page is already created & the browser doesn’t have to do much assembling & rendering work.
- The server-side rendering approach is perfect for delivering static content, such as WordPress blogs. It’s also good for SEO as the crawlers can easily read the generated content.
- However, the modern websites are highly dependent on Ajax. In such websites, content for a particular module or a section of a page has to be fetched & rendered on the fly. Therefore, server-side rendering doesn’t help much. For every Ajax-request, instead of sending just the required content to the client, the approach generates the entire page on the server. This process consumes unnecessary bandwidth & also fails to provide a smooth user experience.
- A big downside to this is once the number of concurrent users on the website rises, it puts an unnecessary load on the server.
- Client-side rendering works best for modern dynamic Ajax-based websites. Though we can leverage a hybrid approach, to get the most out of both techniques. We can use server-side rendering for the home page & for the other static content on our website & use client-side rendering for the dynamic pages.

16. What is Scalability?
- Scalability means the ability of the application to handle & withstand increased workload without sacrificing the latency.
- Latency is the amount of time a system takes to respond to a user request. Let’s say you send a request to an app to fetch an image & the system takes 2 seconds to respond to your request. The latency of the system is 2 seconds.
- Minimum latency is what efficient software systems strive for. No matter how much the traffic load on a system builds up, the latency should not go up. This is what scalability is.
- If the latency remains the same, we can say yeah, the application scaled well with the increased load & is highly scalable.
- Latency is measured as the time difference between the action that the user takes on the website, it can be an event like the click of a button, & the system response in reaction to that event.
- This latency is generally divided into two parts:
    Network Latency
    Application Latency
- Network Latency is the amount of time that the network takes for sending a data packet from point A to point B. The network should be efficient enough to handle the increased traffic load on the website. To cut down the network latency, businesses use CDN & try to deploy their servers across the globe as close to the end-user as possible.
- Application Latency is the amount of time the application takes to process a user request. There are more than a few ways to cut down the application latency. The first step is to run stress & load tests on the application & scan for the bottlenecks that slow down the system as a whole. I’ve talked more about it in the upcoming lesson.
- We can realize the importance of low latency by the fact that Huawei & Hibernia Atlantic in the year 2011 started laying a fibre-optic link cable across the Atlantic Ocean between London & Newyork, that was estimated having a cost of approx. $300M, just to save traders 6 milliseconds of latency.

17. Types of Scalability
- Vertical Scaling
- Horizontal Scaling
- Cloud Elasiticity: The biggest reason why cloud computing got so popular in the industry is the ability to scale up & down dynamically. The ability to use & pay only for the resources required by the website became a trend for obvious reasons.
- Having multiple server nodes on the backend also helps with the website staying alive online all the time even if a few server nodes crash. This is known as High Availability.

18. Which Scalability Approach Is Right For Your App?
- Vertical scaling for obvious reasons is simpler in comparison to scaling horizontally as we do not have to touch the code or make any complex distributed system configurations. It takes much less administrative, monitoring, management efforts as opposed to when managing a distributed environment.
- A major downside of vertical scaling is availability risk. The servers are powerful but few in number, there is always a risk of them going down & the entire website going offline which doesn’t happen when the system is scaled horizontally. It becomes more highly available.
- If you need to run the code in a distributed environment, it needs to be stateless. There should be no state in the code.
- No static instances in the class. Static instances hold application data & if a particular server goes down all the static data/state is lost. The app is left in an inconsistent state.
- Rather, use a persistent memory like a key-value store to hold the data & to remove all the state/static variable from the class. This is why functional programming got so popular with distributed systems. The functions don’t retain any state.
- Development teams today are adopting a distributed micro-services architecture right from the start & the workloads are meant to be deployed on the cloud. So, inherently the workloads are horizontally scaled out on the fly.

19. Primary Bottlenecks that Hurt the Scalability Of Our Application
    Database
    Application Architecture
    Not Using Caching In the Application Wisely
    Inefficient Configuration & Setup of Load Balancers
    Adding Business Logic to the Database
    Not Picking the Right Database
    At the Code Level 
- Just one server been given the onus of handling the data requests from all the server nodes of the workload. This scenario is a bottleneck. The server nodes work well, handle millions of requests at a point in time efficiently, still, the response time of these requests & the latency of the application is very high due to the presence of a single database. There is only so much it can handle.
- Just like the workload scalability, the database needs to be scaled well.
- Make wise use of database partitioning, sharding, use multiple database servers to make the module efficient.
- A poorly designed application’s architecture can become a major bottleneck as a whole. A common architectural mistake is not using asynchronous processes & modules where ever required rather all the processes are scheduled sequentially.
- For instance, if a user uploads a document on the portal, tasks such as sending a confirmation email to the user, sending a notification to all of the subscribers/listeners to the upload event should be done asynchronously.
- These tasks should be forwarded to a messaging server as opposed to doing it all sequentially & making the user wait for everything.
- Caching can be deployed at several layers of the application & it speeds up the response time by notches. It intercepts all the requests going to the database, reducing the overall load on it.
- Use caching exhaustively throughout the application to speed up things significantly.
- Load balancers are the gateway to our application. Using too many or too few of them impacts the latency of our application.
- No matter what justification anyone provides, I’ve never been a fan of adding business logic to the database.
- The database is just not the place to put business logic. Not only it makes the whole application tightly coupled. It puts unnecessary load on it.
- Imagine when migrating to a different database, how much code refactoring it would require.
- Picking the right database technology is vital for businesses. Need transactions & strong consistency? Pick a Relational Database. If you can do without strong consistency rather need horizontal scalability on the fly pick a NoSQL database.
- Trying to pull off things with a not so suitable tech always has a profound impact on the latency of the entire application in negative ways.
- Using unnecessary loops, nested loops.
- Writing tightly coupled code.
- Not paying attention to the Big-O complexity while writing the code. Be ready to do a lot of firefighting in production.

20. How To Improve & Test the Scalability Of Our Application?
    Tuning The Performance Of The Application – Enabling It To Scale Better
        Profiling
        Caching
        CDN (Content Delivery Network)
        Data Compression
        Avoid Unnecessary Client Server Requests
    Testing the Scalability Of Our Application
- The application’s performance is directly proportional to scalability. If an application is not performant it will certainly not scale well.
- Run application profiler, code profiler. See which processes are taking too long, eating up too much resources. Find out the bottlenecks. Get rid of them. Profiling is the dynamic analysis of our code. It helps us measure the space and the time complexity of our code & enables us to figure out issues like concurrency errors, memory errors & robustness & safety of the program.
- Cache wisely. Cache everywhere. Cache all the static content. Hit the database only when it is really required. Try to serve all the read requests from the cache. Use a write-through cache.
- Use a CDN. Using a CDN further reduces the latency of the application due to the proximity of the data from the requesting user.
- Compress data. Use apt compression algorithms to compress data. Store data in the compressed form. As compressed data consumes less bandwidth, consequently, the download speed of the data on the client will be faster.
- Avoid unnecessary round trips between the client & server. Try to club multiple requests into one.
- During the scalability testing, different system parameters are taken into account such as the CPU usage, network bandwidth consumption, throughput, the number of requests processed within a stipulated time, latency, memory usage of the program, end-user experience when the system is under heavy load etc.
- In this testing phase, simulated traffic is routed to the system, to study how the system behaves under the heavy load, how the application scales under the heavy load. Contingencies are planned for unforeseen situations.
- Several load & stress tests are run on the application. Tools like JMeter are pretty popular for running concurrent user test on the application if you are working on a Java ecosystem. There are a lot of cloud-based testing tools available that help us simulate tests scenarios just with a few mouse clicks.
- In the industry tech like Cadvisor, Prometheus and Grafana are pretty popular for tracking the system via web-based dashboards.

21. What Is High Availability?
- High availability also known as HA is the ability of the system to stay online despite having failures at the infrastructural level in real-time. High availability ensures the uptime of the service much more than the normal time. It improves the reliability of the system, ensures minimum downtime. 
- To meet the high availability requirements systems are designed to be fault-tolerant, their components are made redundant.

22. Reasons For System Failures
    Software Crashes
    Hardware Failures
    Human Errors
    Planned Downtime
- Software running on cloud nodes crash unpredictably, along with it they take down the entire node.     
- Overloaded CPU, RAM, hard disk failures, nodes going down. Network outages.
- Google made a tiny network configuration error & it took down almost half of the internet in Japan.
- Besides the unplanned crashes, there are planned down times which involve routine maintenance operations, patching of software, hardware upgrades etc.

23. Achieving High Availability - Fault Tolerance
- Fault tolerance is the ability of the system to stay up despite taking hits.
- A fault-tolerant system is equipped to handle faults. Being fault-tolerant is an essential element in designing life-critical systems.
- A few of the instances/nodes, out of several, running the service go offline & bounce back all the time. In case of these internal failures, the system could work at a reduced level but it will not go down entirely.
- A very basic example of a system being fault-tolerant is a social networking application. In the case of backend node failures, a few services of the app such as image upload, post likes etc. may stop working. But the application as a whole will still be up. This approach is also technically known as Fail Soft.
- To achieve high availability at the application level, the entire massive service is architecturally broken down into smaller loosely coupled services called the micro-services.

24. Redundancy

    Redundancy – Active-Passive HA Mode
    Getting Rid Of Single Points Of Failure
    Monitoring & Automation

- Redundancy is duplicating the components or instances & keeping them on standby to take over in case the active instances go down. It’s the fail-safe, backup mechanism.
- In the diagram, you can see the instances active & on standby. The standby instances take over in case any of the active instances goes down.
- This approach is also known as Active-Passive HA mode. An initial set of nodes are active & a set of redundant nodes are passive, on standby. Active nodes get replaced by passive nodes, in case of failures.
- There are systems like GPS, aircrafts, communication satellites which have zero downtime. The availability of these systems is ensured by making the components redundant.
- Distributed systems got so popular solely due to the reason that with them, we could get rid of the single points of failure present in a monolithic architecture.
- A large number of distributed nodes work in conjunction with each other to achieve a single synchronous application state.
- When so many redundant nodes are deployed, there are no single points of failure in the system. In case a node goes down redundant nodes take its place. Thus, the system as a whole remains unimpacted.
- Single points of failure at the application level mean bottlenecks. We should detect bottlenecks in performance testing & get rid of them as soon as we can.
- Systems should be well monitored in real-time to detect any bottlenecks or single point of failures. Automation enables the instances to self-recover without any human intervention. It gives the instances the power of self-healing.
- Also, the systems become intelligent enough to add or remove instances on the fly as per the requirements.
- Since the most common cause of failures is human error, automation helps cut down failures to a big extent.

25. Replication

	Replication – Active-Active HA Mode
	Geographical Distribution of Workload

- Replication means having a number of similar nodes running the workload together. There are no standby or passive instances. When a single or a few nodes go down, the remaining nodes bear the load of the service. Think of this as load balancing.	
- This approach is also known as the Active-Active High Availability mode. In this approach, all the components of the system are active at any point in time.
- As a contingency for natural disasters, data centre regional power outages & other big-scale failures, workloads are spread across different data centres across the world in different geographical zones.
- This avoids the single point of failure thing in context to a data centre. Also, the latency is reduced by quite an extent due to the proximity of data to the user.
- Businesses often use multi-cloud platforms to deploy their workloads which ensures further availability. If things go south with one cloud provider, they have another to fail back over.

26. High Availability Clustering
- A High Availability cluster also known as the Fail-Over cluster contains a set of nodes running in conjunction with each other that ensures high availability of the service.
- The nodes in the cluster are connected by a private network called the Heartbeat network that continuously monitors the health and the status of each node in the cluster.
- A single state across all the nodes in a cluster is achieved with the help of a shared distributed memory & a distributed co-ordination service like the Zookeeper.
- To ensure the availability, HA clusters use several techniques such as Disk mirroring/RAID Redundant Array Of Independent Disks, redundant network connections, redundant electrical power etc. The network connections are made redundant so if the primary network goes down the backup network takes over.
- Multiple HA clusters run together in one geographical zone ensuring minimum downtime & continual service.

27. Introduction To Load Balancing
- Load balancing is vital in enabling our service to scale well, with the increase in the traffic load, as well as stay highly available. Load balancing is facilitated by load balancers, that makes them a key component in the web application architecture.
- Load balancers distribute heavy traffic load across the servers running in the cluster based on several different algorithms. This averts the risks of convergence of all the traffic on the service to a single or a few machines in the cluster.
- If the entire traffic on the service is converged only to a few machines this will not only overload them resulting in the increase in the latency of the application, killing its performance. But will also eventually bring them down.
- Load balancing helps us avoid all this mess. Amidst processing a user request if a server goes down, the load balancer automatically routes the future requests to other up and running servers in the cluster thus enabling the service as a whole to stay available.
- Load balancers can also be setup to efficiently manage traffic directed towards any component of the application be it the backend application server, database component, message queue or any other. This is done to uniformly spread the request load across the machines in the clusters powering that respective component.
- In order to intelligently route all the user requests to the running servers in the cluster, a load balancer should be well aware of their running status.
- To ensure that the user request is always routed to the machine that is up and running, load balancers regularly perform health checks of the machines in the cluster.
- Ideally, a load balancer maintains a list of machines that are up and running in the cluster in real-time & the user requests are forwarded to only those machines that are in service. If a machine goes down it is removed from the list.
- Machines that are up and running in the cluster are known as in-service machines. And the servers that are down are known as out of service instances.
- Just for the record – Node, Server, Server Node, Instance, Machine they all mean the same thing & can be used interchangeably.

28. Understanding DNS 
- Every machine that is online & is a part of world wide web www has a unique IP address that enables it to be contacted by other machines on the web using that particular IP address.
- Domain name system commonly known as DNS is a system that averts the need to remember long IP addresses to visit a website by mapping easy to remember domain names to IP addresses.
- When a user types in the url of the website in their browser and hits enter. This event is known as DNS querying.
- There are four key components, i.e. a group of servers, that make up the DNS infrastructure. Those are -

    DNS Recursive Nameserver aka DNS Resolver
    Root Nameserver
    Top-Level Domain Nameserver
    Authoritative Nameserver
- So, when the user hits enter after typing in the domain name into their browser, the browser sends a request to the DNS Recursive nameserver which is also known as the DNS Resolver.
- The role of DNS Resolver is to receive the client request and forward it to the Root nameserver to get the address of the Top-Level domain nameserver.
- DNS Recursive nameserver is generally managed by our ISP Internet service provider. The whole DNS system is a distributed system setup in large data centers managed by internet service providers.
- These data centers contain clusters of servers that are optimized to process DNS queries in minimal time that is in milliseconds.
- So, once the DNS Resolver forwards the request to the Root nameserver, the Root nameserver returns the address of the Top-Level domain nameserver in response. As an example, the top-level domain for amazon.com is .com.
- Once the DNS Resolver receives the address of the top-level domain nameserver, it sends a request to it to fetch the details of the domain name. Top level domain nameservers hold the data for domains using their top-level domains.
- For instance, .com top-level domain nameserver will contain information on domains using .com. Similarly, a .edu top-level domain nameserver will hold information on domains using .edu.
- Once the top-level domain name server receives the request from the Resolver, it returns the IP address of amazon.com domain name server.
- amazon.com domain name server is the last server in the DNS query lookup process. It is the nameserver responsible for the amazon.com domain & is also known as the Authoritative nameserver. This nameserver is owned by the owner of the domain name.
- DNS Resolver then fires a query to the authoritative nameserver & it then returns the IP address of amazon.com website to the DNS Resolver. DNS Resolver caches the data and forwards it to the client.
- On receiving the response, the browser sends a request to the IP address of the amazon.com website to fetch data from their servers.
- Often all this DNS information is cached and the DNS servers don’t have to do so much rerouting every time a client requests an IP of a certain website.
- DNS information of websites that we visit also gets cached in our local machines that is our browsing devices with a TTL Time To Live.
- All the modern browsers do this automatically to cut down the DNS query lookup time when revisiting a website.
-     




















