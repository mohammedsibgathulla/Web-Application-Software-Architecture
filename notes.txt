1. Single Tier Applications
- A single-tier application is an application where the user interface, backend business logic & the database all reside in the same machine.
- The main upside of single-tier applications is they have no network latency since every component is located on the same machine. This adds up to the performance of the software.

2. Two Tier Application 
- A Two-tier application involves a client and a server. The client would contain the user interface & the business logic in one machine. And the backend server would be the database running on a different machine. The database server is hosted by the business & has control over it.

3. Three Tier applications
- In a three-tier application, the user interface, application logic & the database all lie on different machines & thus have different tiers. They are physically separated.

4. N Tier Applications
- An N-tier application is an application which has more than three components involved. For example 
  - Cache
  - Message queues for asynchronous behaviour
  - Load balancers
  - Search servers for searching through massive amounts of data
  - Components involved in processing massive amounts of data
  - Components running heterogeneous tech commonly known as web services etc.
- Note: There is another name for n-tier apps, the “distributed applications”. But, I think it’s not safe to use the word “distributed” yet, as the term distributed brings along a lot of complex stuff with it. It would rather confuse us than help. Though I will discuss the distributed architecture in this course, for now, we will just stick with the term N-tier applications.
- Why the need for so many tiers? 
- Two software design principles that are key to explaining this are the Single Responsibility Principle & the Separation of Concerns.
- Single Responsibility Principle simply means giving one, just one responsibility to a component & letting it execute it with perfection. Be it saving data, running the application logic or ensuring the delivery of the messages throughout the system.
- Separation of concerns kind of means the same thing, be concerned about your work only & stop worrying about the rest of the stuff. Keeping the components separate makes them reusable. Different services can use the same database, the messaging server or any component as long as they are not tightly coupled with each other.
- Note: Don’t confuse tiers with the layers of the application. Some prefer to use them interchangeably. But in the industry layers of an application typically means the user interface layer, business layer, service layer, or the data access layer.

5. What Is Web Architecture?
- Web architecture involves multiple components like database, message queue, cache, user interface & all running in conjunction with each other to form an online service.

6. Client Server Architecture
- The architecture works on a request-response model. The client sends the request to the server for information & the server responds with it.
- A very small percent of the business websites and applications use the peer to peer architecture, which is different from the client-server.
- Every website you browse, be it a Wordpress blog or a web application like Facebook, Twitter or your banking app is built on the client-server architecture.

7. Client
- The client holds our user interface. The user interface is the presentation part of the application. It’s written in Html, JavaScript, CSS and is responsible for the look & feel of the application.
- The user interface runs on the client. The client can be a mobile app, a desktop or a tablet like an iPad. It can also be a web-based console, running commands to interact with the backend server.

8. Types of Client
- Thin Client & Thick Client
- Thin Client is the client which holds just the user interface of the application. It has no business logic of any sort. For every action, the client sends a request to the backend server. Just like in a three-tier application.
- On the contrary, the thick client holds all or some part of the business logic. These are the two-tier applications. We’ve already gone through this if you remember.

9. Server
- The primary task of a web server is to receive the requests from the client & provide the response after executing the business logic based on the request parameters received from the client.
- Every service, running online, needs a server to run. Servers running web applications are commonly known as the application servers.
- Besides the application servers, there are other kinds of servers too with specific tasks assigned to them such as the:	
    Proxy server
    Mail server
    File server
    Virtual server
- All the components of a web application need a server to run. Be it a database, a message queue, a cache or any other component. In modern application development, even the user interface is hosted separately on a dedicated server.

10. Communication Between the Client & the Server
- The client & the server have a request-response model. The client sends the request & the server responds with the data.
- The entire communication happens over the HTTP protocol. It is the protocol for data exchange over the World Wide Web. HTTP protocol is a request-response protocol that defines how information is transmitted across the web.
- It’s a stateless protocol, every process over HTTP is executed independently & has no knowledge of previous processes.
- Speaking from the context of modern N-tier web applications, every client has to hit a REST end-point to fetch the data from the backend.
- The backend application code has a REST-API implemented which acts as an interface to the outside world requests. Every request be it from the client written by the business or the third-party developers which consume our data have to hit the REST-endpoints to fetch the data.

11. HTTP Push & Pull - Introduction
- For every response, there has to be a request first. The client sends the request & the server responds with the data. This is the default mode of HTTP communication, called the HTTP PULL mechanism.    
- In HTTP PUSH based mechanism the client sends the request for particular information to the server, just for the first time, & after that the server keeps pushing the new updates to the client whenever they are available. The client doesn’t have to worry about sending requests to the server, for data, every now & then. This saves a lot of network bandwidth & cuts down the load on the server by notches. This is also known as a Callback. Client phones the server for information. The server responds, Hey!! I don’t have the information right now but I’ll call you back whenever it is available.
- There are multiple technologies involved in the HTTP Push based mechanism such as:	
    Ajax Long polling
    Web Sockets
    HTML5 Event Source
    Message Queues
    Streaming over HTTP

12. HTTP Pull - Polling with Ajax
- There are two ways of pulling/fetching data from the server. The first is sending an HTTP GET request to the server manually by triggering an event, like by clicking a button or any other element on the web page.
- The other is fetching data dynamically at regular intervals by using AJAX without any human intervention. AJAX is commonly used with the Jquery framework to implement the asynchronous behaviour on the UI. 
- This dynamic technique of requesting information from the server after regular intervals is known as Polling.

13. HTTP Push
- The above mechanism has the following
    Time To Live (TTL)
    Persistent Connection
    Heartbeat Interceptors
    Resource Intensive
- In the regular client-server communication, which is HTTP PULL, there is a Time to Live (TTL) for every request. It could be 30 secs to 60 secs, varies from browser to browser.
- If the client doesn’t receive a response from the server within the TTL, the browser kills the connection & the client has to re-send the request hoping it would receive the data from the server before the TTL ends this time.
- Open connections consume resources & there is a limit to the number of open connections a server can handle at one point in time. If the connections don’t close & new ones are being introduced, over time, the server will run out of memory. Hence, the TTL is used in client-server communication.
- But what if we are certain that the response will take more time than the TTL set by the browser?. In this case, we need a Persistent Connection between the client and the server. 
- A persistent connection is a network connection between the client & the server that remains open for further requests & the responses, as opposed to being closed after a single communication. It facilitates HTTP Push-based communication between the client and the server.    
- Now you might be wondering how is a persistent connection possible if the browser kills the open connections to the server every X seconds?. The connection between the client and the server stays open with the help of Heartbeat Interceptors.
- These are just blank request responses between the client and the server to prevent the browser from killing the connection.
- Persistent connections consume a lot of resources in comparison to the HTTP Pull behaviour. But there are use cases where establishing a persistent connection is vital to the feature of an application.For instance, a browser-based multiplayer game has a pretty large amount of request-response activity within a certain time in comparison to a regular web application.
- It would be apt to establish a persistent connection between the client and the server from a user experience standpoint.
- Long opened connections can be implemented by multiple techniques such as Ajax Long Polling, Web Sockets, Server-Sent Events etc.

14. HTTP Push-Based Technologies
- The below are the HTTP Push-Based technologies  
    Web Sockets
    AJAX – Long Polling
    HTML5 Event Source API & Server Sent Events
    Streaming Over HTTP
- A Web Socket connection is ideally preferred when we need a persistent bi-directional low latency data flow from the client to server & back.
- Typical use-cases of these are messaging, chat applications, real-time social streams & browser-based massive multiplayer games which have quite a number of read writes in comparison to a regular web app.
- With Web Sockets, we can keep the client-server connection open as long as we want. Have bi-directional data? Go ahead with Web Sockets. One more thing, Web Sockets tech doesn’t work over HTTP. It runs over TCP. The server & the client should both support web sockets or else it won’t work.
- Long Polling lies somewhere between Ajax & Web Sockets. In this technique instead of immediately returning the response, the server holds the response until it finds an update to be sent to the client. The connection in long polling stays open a bit longer in comparison to polling. The server doesn’t return an empty response. If the connection breaks, the client has to re-establish the connection to the server. The upside of using this technique is that there are quite a smaller number of requests sent from the client to the server, in comparison to the regular polling mechanism. This reduces a lot of network bandwidth consumption. Long polling can be used in simple asynchronous data fetch use cases when you do not want to poll the server every now & then.
- The Server-Sent Events implementation takes a bit of a different approach. Instead of the client polling for data, the server automatically pushes the data to the client whenever the updates are available. The incoming messages from the server are treated as Events. Via this approach, the servers can initiate data transmission towards the client once the client has established the connection with an initial request.
- This helps in getting rid of a huge number of blank request-response cycles cutting down the bandwidth consumption by notches.
- To implement server-sent events, the backend language should support the technology & on the UI HTML5 Event source API is used to receive the data in-coming from the backend.
- An important thing to note here is that once the client establishes a connection with the server, the data flow is in one direction only, that is from the server to the client.
- SSE is ideal for scenarios such as a real-time feed like that of Twitter, displaying stock quotes on the UI, real-time notifications etc.
- Streaming Over HTTP is ideal for cases where we need to stream large data over HTTP by breaking it into smaller chunks. This is possible with HTML5 & a JavaScript Stream API.
- The technique is primarily used for streaming multimedia content, like large images, videos etc, over HTTP. Due to this, we can watch a partially downloaded video as it continues to download, by playing the downloaded chunks on the client. This helps them figure when the stream begins & ends over an HTTP request-response model.

15. Client-Side Vs Server-Side Rendering
- When a user requests a web page from the server & the browser receives the response. It has to render the response on the window in the form of an HTML page. For this, the browser has several components, such as the:
    Browser engine
    Rendering engine
    JavaScript interpreter
    Networking & the UI backend
    Data storage etc.
- The rendering engine constructs the DOM tree, renders & paints the construction. And naturally, all this activity needs a bit of time.
- To avoid all this rendering time on the client, developers often render the UI on the server, generate HTML there & directly send the HTML page to the UI.
- This technique is known as the Server-side rendering. It ensures faster rendering of the UI, averting the UI loading time in the browser window since the page is already created & the browser doesn’t have to do much assembling & rendering work.
- The server-side rendering approach is perfect for delivering static content, such as WordPress blogs. It’s also good for SEO as the crawlers can easily read the generated content.
- However, the modern websites are highly dependent on Ajax. In such websites, content for a particular module or a section of a page has to be fetched & rendered on the fly. Therefore, server-side rendering doesn’t help much. For every Ajax-request, instead of sending just the required content to the client, the approach generates the entire page on the server. This process consumes unnecessary bandwidth & also fails to provide a smooth user experience.
- A big downside to this is once the number of concurrent users on the website rises, it puts an unnecessary load on the server.
- Client-side rendering works best for modern dynamic Ajax-based websites. Though we can leverage a hybrid approach, to get the most out of both techniques. We can use server-side rendering for the home page & for the other static content on our website & use client-side rendering for the dynamic pages.

16. What is Scalability?
- Scalability means the ability of the application to handle & withstand increased workload without sacrificing the latency.
- Latency is the amount of time a system takes to respond to a user request. Let’s say you send a request to an app to fetch an image & the system takes 2 seconds to respond to your request. The latency of the system is 2 seconds.
- Minimum latency is what efficient software systems strive for. No matter how much the traffic load on a system builds up, the latency should not go up. This is what scalability is.
- If the latency remains the same, we can say yeah, the application scaled well with the increased load & is highly scalable.
- Latency is measured as the time difference between the action that the user takes on the website, it can be an event like the click of a button, & the system response in reaction to that event.
- This latency is generally divided into two parts:
    Network Latency
    Application Latency
- Network Latency is the amount of time that the network takes for sending a data packet from point A to point B. The network should be efficient enough to handle the increased traffic load on the website. To cut down the network latency, businesses use CDN & try to deploy their servers across the globe as close to the end-user as possible.
- Application Latency is the amount of time the application takes to process a user request. There are more than a few ways to cut down the application latency. The first step is to run stress & load tests on the application & scan for the bottlenecks that slow down the system as a whole. I’ve talked more about it in the upcoming lesson.
- We can realize the importance of low latency by the fact that Huawei & Hibernia Atlantic in the year 2011 started laying a fibre-optic link cable across the Atlantic Ocean between London & Newyork, that was estimated having a cost of approx. $300M, just to save traders 6 milliseconds of latency.

17. Types of Scalability
- Vertical Scaling
- Horizontal Scaling
- Cloud Elasiticity: The biggest reason why cloud computing got so popular in the industry is the ability to scale up & down dynamically. The ability to use & pay only for the resources required by the website became a trend for obvious reasons.
- Having multiple server nodes on the backend also helps with the website staying alive online all the time even if a few server nodes crash. This is known as High Availability.

18. Which Scalability Approach Is Right For Your App?
- Vertical scaling for obvious reasons is simpler in comparison to scaling horizontally as we do not have to touch the code or make any complex distributed system configurations. It takes much less administrative, monitoring, management efforts as opposed to when managing a distributed environment.
- A major downside of vertical scaling is availability risk. The servers are powerful but few in number, there is always a risk of them going down & the entire website going offline which doesn’t happen when the system is scaled horizontally. It becomes more highly available.
- If you need to run the code in a distributed environment, it needs to be stateless. There should be no state in the code.
- No static instances in the class. Static instances hold application data & if a particular server goes down all the static data/state is lost. The app is left in an inconsistent state.
- Rather, use a persistent memory like a key-value store to hold the data & to remove all the state/static variable from the class. This is why functional programming got so popular with distributed systems. The functions don’t retain any state.
- Development teams today are adopting a distributed micro-services architecture right from the start & the workloads are meant to be deployed on the cloud. So, inherently the workloads are horizontally scaled out on the fly.

19. Primary Bottlenecks that Hurt the Scalability Of Our Application
    Database
    Application Architecture
    Not Using Caching In the Application Wisely
    Inefficient Configuration & Setup of Load Balancers
    Adding Business Logic to the Database
    Not Picking the Right Database
    At the Code Level 
- Just one server been given the onus of handling the data requests from all the server nodes of the workload. This scenario is a bottleneck. The server nodes work well, handle millions of requests at a point in time efficiently, still, the response time of these requests & the latency of the application is very high due to the presence of a single database. There is only so much it can handle.
- Just like the workload scalability, the database needs to be scaled well.
- Make wise use of database partitioning, sharding, use multiple database servers to make the module efficient.
- A poorly designed application’s architecture can become a major bottleneck as a whole. A common architectural mistake is not using asynchronous processes & modules where ever required rather all the processes are scheduled sequentially.
- For instance, if a user uploads a document on the portal, tasks such as sending a confirmation email to the user, sending a notification to all of the subscribers/listeners to the upload event should be done asynchronously.
- These tasks should be forwarded to a messaging server as opposed to doing it all sequentially & making the user wait for everything.
- Caching can be deployed at several layers of the application & it speeds up the response time by notches. It intercepts all the requests going to the database, reducing the overall load on it.
- Use caching exhaustively throughout the application to speed up things significantly.
- Load balancers are the gateway to our application. Using too many or too few of them impacts the latency of our application.
- No matter what justification anyone provides, I’ve never been a fan of adding business logic to the database.
- The database is just not the place to put business logic. Not only it makes the whole application tightly coupled. It puts unnecessary load on it.
- Imagine when migrating to a different database, how much code refactoring it would require.
- Picking the right database technology is vital for businesses. Need transactions & strong consistency? Pick a Relational Database. If you can do without strong consistency rather need horizontal scalability on the fly pick a NoSQL database.
- Trying to pull off things with a not so suitable tech always has a profound impact on the latency of the entire application in negative ways.
- Using unnecessary loops, nested loops.
- Writing tightly coupled code.
- Not paying attention to the Big-O complexity while writing the code. Be ready to do a lot of firefighting in production.

20. How To Improve & Test the Scalability Of Our Application?
    Tuning The Performance Of The Application – Enabling It To Scale Better
        Profiling
        Caching
        CDN (Content Delivery Network)
        Data Compression
        Avoid Unnecessary Client Server Requests
    Testing the Scalability Of Our Application
- The application’s performance is directly proportional to scalability. If an application is not performant it will certainly not scale well.
- Run application profiler, code profiler. See which processes are taking too long, eating up too much resources. Find out the bottlenecks. Get rid of them. Profiling is the dynamic analysis of our code. It helps us measure the space and the time complexity of our code & enables us to figure out issues like concurrency errors, memory errors & robustness & safety of the program.
- Cache wisely. Cache everywhere. Cache all the static content. Hit the database only when it is really required. Try to serve all the read requests from the cache. Use a write-through cache.
- Use a CDN. Using a CDN further reduces the latency of the application due to the proximity of the data from the requesting user.
- Compress data. Use apt compression algorithms to compress data. Store data in the compressed form. As compressed data consumes less bandwidth, consequently, the download speed of the data on the client will be faster.
- Avoid unnecessary round trips between the client & server. Try to club multiple requests into one.
- During the scalability testing, different system parameters are taken into account such as the CPU usage, network bandwidth consumption, throughput, the number of requests processed within a stipulated time, latency, memory usage of the program, end-user experience when the system is under heavy load etc.
- In this testing phase, simulated traffic is routed to the system, to study how the system behaves under the heavy load, how the application scales under the heavy load. Contingencies are planned for unforeseen situations.
- Several load & stress tests are run on the application. Tools like JMeter are pretty popular for running concurrent user test on the application if you are working on a Java ecosystem. There are a lot of cloud-based testing tools available that help us simulate tests scenarios just with a few mouse clicks.
- In the industry tech like Cadvisor, Prometheus and Grafana are pretty popular for tracking the system via web-based dashboards.

21. What Is High Availability?
- High availability also known as HA is the ability of the system to stay online despite having failures at the infrastructural level in real-time. High availability ensures the uptime of the service much more than the normal time. It improves the reliability of the system, ensures minimum downtime. 
- To meet the high availability requirements systems are designed to be fault-tolerant, their components are made redundant.













